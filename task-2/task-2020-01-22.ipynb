{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk==3.4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from this link:\n",
    "    \n",
    "    https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous day we worked with already preprocessed data for us.  \n",
    "This day try to make this preprocessing by ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text lowercasing: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a column 'comment_text_lower' in a dataframe, and make all of the text from the column 'comment_text' copied to the 'comment_text_lower' column, but lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_text_lower'] = df['comment_text'].str.lower()\n",
    "### Your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_lower</th>\n",
       "      <th>comment_text_tokenized_space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>[d'aww!, he, matches, this, background, colour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>[hey, man,, i'm, really, not, trying, to, edit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>[\", more, i, can't, make, any, real, suggestio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>[you,, sir,, are, my, hero., any, chance, you,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                  comment_text_lower  \\\n",
       "0  explanation\\nwhy the edits made under my usern...   \n",
       "1  d'aww! he matches this background colour i'm s...   \n",
       "2  hey man, i'm really not trying to edit war. it...   \n",
       "3  \"\\nmore\\ni can't make any real suggestions on ...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                        comment_text_tokenized_space  \n",
       "0  [explanation, why, the, edits, made, under, my...  \n",
       "1  [d'aww!, he, matches, this, background, colour...  \n",
       "2  [hey, man,, i'm, really, not, trying, to, edit...  \n",
       "3  [\", more, i, can't, make, any, real, suggestio...  \n",
       "4  [you,, sir,, are, my, hero., any, chance, you,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a column 'comment_text_tokenized_space' in a dataframe, and make all of the text from the column 'comment_text' copied to the 'comment_text_tokenized_space' column, but lowercased tokenized by space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_text_tokenized_space'] = df['comment_text_lower'].str.split(r'[\\n ]+')\n",
    "### Your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_lower</th>\n",
       "      <th>comment_text_tokenized_space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>[d'aww!, he, matches, this, background, colour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>[hey, man,, i'm, really, not, trying, to, edit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>[\", more, i, can't, make, any, real, suggestio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>[you,, sir,, are, my, hero., any, chance, you,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                  comment_text_lower  \\\n",
       "0  explanation\\nwhy the edits made under my usern...   \n",
       "1  d'aww! he matches this background colour i'm s...   \n",
       "2  hey man, i'm really not trying to edit war. it...   \n",
       "3  \"\\nmore\\ni can't make any real suggestions on ...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                        comment_text_tokenized_space  \n",
       "0  [explanation, why, the, edits, made, under, my...  \n",
       "1  [d'aww!, he, matches, this, background, colour...  \n",
       "2  [hey, man,, i'm, really, not, trying, to, edit...  \n",
       "3  [\", more, i, can't, make, any, real, suggestio...  \n",
       "4  [you,, sir,, are, my, hero., any, chance, you,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There a lot of words tokenized by space, but they contains additional punctuation characters. Let's try to delete them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load punctuation\n",
    "\n",
    "def clean_token(token): \n",
    "    '''\n",
    "    Args: token: str \n",
    "    Returns: token: str \n",
    "    \n",
    "    This function deletes all of the punctuation characters \n",
    "    in the token and returns the cleaned one \n",
    "    '''\n",
    "    #chars_cleaned =  ### Your code here\n",
    "    return re.sub(r'[0-9\\W]*','',  token)\n",
    "#    return re.sub(r'\\W*','',re.sub(r'[0-9]*', '', token)) #\"\".join(chars_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use method apply - read about it more if needed (pandas, df.apply, lambda functions, list of comprehension)\n",
    "df['comment_text_tokenized_space_cleaned'] = df.comment_text_tokenized_space.apply(\n",
    "    lambda x: [clean_token(x[i]) for i in range(len(x))] ### You code here (list of comprehension)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_lower</th>\n",
       "      <th>comment_text_tokenized_space</th>\n",
       "      <th>comment_text_tokenized_space_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>[d'aww!, he, matches, this, background, colour...</td>\n",
       "      <td>[daww, he, matches, this, background, colour, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>[hey, man,, i'm, really, not, trying, to, edit...</td>\n",
       "      <td>[hey, man, im, really, not, trying, to, edit, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>[\", more, i, can't, make, any, real, suggestio...</td>\n",
       "      <td>[, more, i, cant, make, any, real, suggestions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>[you,, sir,, are, my, hero., any, chance, you,...</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                  comment_text_lower  \\\n",
       "0  explanation\\nwhy the edits made under my usern...   \n",
       "1  d'aww! he matches this background colour i'm s...   \n",
       "2  hey man, i'm really not trying to edit war. it...   \n",
       "3  \"\\nmore\\ni can't make any real suggestions on ...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                        comment_text_tokenized_space  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [d'aww!, he, matches, this, background, colour...   \n",
       "2  [hey, man,, i'm, really, not, trying, to, edit...   \n",
       "3  [\", more, i, can't, make, any, real, suggestio...   \n",
       "4  [you,, sir,, are, my, hero., any, chance, you,...   \n",
       "\n",
       "                comment_text_tokenized_space_cleaned  \n",
       "0  [explanation, why, the, edits, made, under, my...  \n",
       "1  [daww, he, matches, this, background, colour, ...  \n",
       "2  [hey, man, im, really, not, trying, to, edit, ...  \n",
       "3  [, more, i, cant, make, any, real, suggestions...  \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a lot of work, yes?   \n",
    "Let's try to use already implemented methods for performing a tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['nltk_tokenized'] = df.comment_text.apply(lambda x: word_tokenize(x))### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_lower</th>\n",
       "      <th>comment_text_tokenized_space</th>\n",
       "      <th>comment_text_tokenized_space_cleaned</th>\n",
       "      <th>nltk_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>[Explanation, Why, the, edits, made, under, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>[d'aww!, he, matches, this, background, colour...</td>\n",
       "      <td>[daww, he, matches, this, background, colour, ...</td>\n",
       "      <td>[D'aww, !, He, matches, this, background, colo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>[hey, man,, i'm, really, not, trying, to, edit...</td>\n",
       "      <td>[hey, man, im, really, not, trying, to, edit, ...</td>\n",
       "      <td>[Hey, man, ,, I, 'm, really, not, trying, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>[\", more, i, can't, make, any, real, suggestio...</td>\n",
       "      <td>[, more, i, cant, make, any, real, suggestions...</td>\n",
       "      <td>[``, More, I, ca, n't, make, any, real, sugges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>[you,, sir,, are, my, hero., any, chance, you,...</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "      <td>[You, ,, sir, ,, are, my, hero, ., Any, chance...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                  comment_text_lower  \\\n",
       "0  explanation\\nwhy the edits made under my usern...   \n",
       "1  d'aww! he matches this background colour i'm s...   \n",
       "2  hey man, i'm really not trying to edit war. it...   \n",
       "3  \"\\nmore\\ni can't make any real suggestions on ...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                        comment_text_tokenized_space  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [d'aww!, he, matches, this, background, colour...   \n",
       "2  [hey, man,, i'm, really, not, trying, to, edit...   \n",
       "3  [\", more, i, can't, make, any, real, suggestio...   \n",
       "4  [you,, sir,, are, my, hero., any, chance, you,...   \n",
       "\n",
       "                comment_text_tokenized_space_cleaned  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [daww, he, matches, this, background, colour, ...   \n",
       "2  [hey, man, im, really, not, trying, to, edit, ...   \n",
       "3  [, more, i, cant, make, any, real, suggestions...   \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...   \n",
       "\n",
       "                                      nltk_tokenized  \n",
       "0  [Explanation, Why, the, edits, made, under, my...  \n",
       "1  [D'aww, !, He, matches, this, background, colo...  \n",
       "2  [Hey, man, ,, I, 'm, really, not, trying, to, ...  \n",
       "3  [``, More, I, ca, n't, make, any, real, sugges...  \n",
       "4  [You, ,, sir, ,, are, my, hero, ., Any, chance...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, see that word_tokenize not only separated the punctuation from the text corretly, but saved the punctuation inside the token (' or -)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ! Modify your function defined previously to save a pucntuation inside the token and delete it only if it glued to the token in the end or in the beginning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_clean_token(token): # модифицырованая функция \"очистки\" токенов сохраняет символы внутри слова\n",
    "    '''\n",
    "    Args: token: str \n",
    "    Returns: token: str \n",
    "    \n",
    "    This function deletes all of the punctuation characters \n",
    "    in the token and returns the cleaned one \n",
    "    '''\n",
    "    x = re.findall(r'[a-z]+.*[a-z]+', token)\n",
    "    if len(x) != 0:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return ''### Your code here\n",
    "\n",
    "def del_words(tokens): # Удаляет пустые, или не несущие информации токены внутри списка\n",
    "    i=0\n",
    "    while i<len(tokens):\n",
    "        if len(tokens[i]) < 4:\n",
    "            del(tokens[i])\n",
    "            i-=1\n",
    "        elif len(tokens[i]) > 9:\n",
    "            del(tokens[i])\n",
    "            i-=1\n",
    "        if len(re.findall(r'\\W', tokens[i])) == 0:\n",
    "            del(tokens[i])\n",
    "        i+=1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [explanation, why, the, edits, made, under, my...\n",
       "1         [d'aww, he, matches, this, background, colour,...\n",
       "2         [hey, man, i'm, really, not, trying, to, edit,...\n",
       "3         [, more, , can't, make, any, real, suggestions...\n",
       "4         [you, sir, are, my, hero, any, chance, you, re...\n",
       "                                ...                        \n",
       "159566    [and, for, the, second, time, of, asking, when...\n",
       "159567    [you, should, be, ashamed, of, yourself, that,...\n",
       "159568    [spitzer, umm, theres, no, actual, article, fo...\n",
       "159569    [and, it, looks, like, it, was, actually, you,...\n",
       "159570    [, and, , , really, don't, think, you, underst...\n",
       "Name: alt_comment_text_tokenized_space_cleaned, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['alt_comment_text_tokenized_space_cleaned'] = df.comment_text_tokenized_space.apply(\n",
    "    lambda x: [new_clean_token(x[i]) for i in range(len(x))]\n",
    "    )\n",
    "df['alt_comment_text_tokenized_space_cleaned']### Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_nested(nested, flatten=[]):\n",
    "    '''\n",
    "    Args: nested list: list ([[a], [b]])\n",
    "    Returns: flatten list: list ([a, b])\n",
    "    '''\n",
    "    for i in nested:\n",
    "        if type(i) == list:\n",
    "            flat_nested(i)\n",
    "        elif i!='':\n",
    "            flatten.append(i)\n",
    "    ### Your code here \n",
    "    return flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Frequency dict will accept only list of tokens, not list of lists of tokens, etc \n",
    "# Flat your list previously if needed \n",
    "fdist = FreqDist(flat_nested(df.comment_text_tokenized_space_cleaned.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 495462),\n",
       " ('to', 296836),\n",
       " ('of', 224020),\n",
       " ('and', 222360),\n",
       " ('a', 214891),\n",
       " ('you', 204556),\n",
       " ('i', 200628),\n",
       " ('is', 175954),\n",
       " ('that', 154297),\n",
       " ('in', 144186),\n",
       " ('it', 129639),\n",
       " ('for', 102441),\n",
       " ('this', 97074),\n",
       " ('not', 93330),\n",
       " ('on', 89444),\n",
       " ('be', 83326),\n",
       " ('as', 77247),\n",
       " ('have', 72171),\n",
       " ('are', 71874),\n",
       " ('your', 63252)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyse the results. What are these words? Have you seen it previously in the 1 task?  \n",
    "Does these words contains a lot of meaningful information? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Это стопслова которые не были удалены\n",
    "### Your COMMENTS here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEcCAYAAAD6GqKbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXxU1d348c83+04CBAhhFUEFFDSAuNUqFazWaltt1Vqx2vpra6tPfWrVp/rYurTa1vpUq1ar1qWLUtuqWFERxJUtiOwgqxDZSQIh+/L9/XHPJJdhkkySmUxIvu/Xa14zc+655547mcz3nuXeK6qKMcYYE2lxsa6AMcaY7skCjDHGmKiwAGOMMSYqLMAYY4yJCgswxhhjosICjDHGmKhIiHUFuoq+ffvqsGHD2r1+ZWUlqampEctnZVqZVqaV2RXLDLZkyZK9qpobcqGq2kOVgoIC7YjCwsKI5rMyrUwr08rsimUGAwq1md9V6yIzxhgTFRZgjDHGREVUA4yIbBGRFSLysYgUurTeIjJbRNa75xxf/ltFZIOIrBORab70AlfOBhF5UETEpSeLyAsufaGIDPOtM91tY72ITI/mfhpjjDlcZ7RgzlLV8ao6wb2/BZijqiOBOe49IjIauBQYA5wLPCIi8W6dR4FrgZHuca5LvwYoUdWjgQeA+1xZvYE7gJOBScAd/kBmjDEm+mLRRXYh8Ix7/QxwkS/9eVWtVtXNwAZgkojkAVmqOt8NKD0btE6grBeBKa51Mw2YrarFqloCzKYpKBljjOkE0Q4wCrwpIktE5FqX1l9VdwC4534uPR/Y5lu3yKXlu9fB6Yeso6p1wH6gTwtlGWOM6SSiUbxcv4gMVNXtItIPrxXxI+AVVc325SlR1RwReRiYr6p/celPAq8BW4FfqeoXXPoZwE9V9QIRWQVMU9Uit2wjXpfY1UCyqt7t0m8HKlT1/qD6XYvX9UZeXl7BzJkz27yPZTUNbN1fh9RVMzovs9X8FRUVpKWlhVV2uHmtTCvTyrQyO6vMYBMmTFjiGwI5VHPzlyP9AH4O/ARYB+S5tDxgnXt9K3CrL/8bwCkuz1pf+mXAY/487nUCsBcQfx637DHgspbq197zYP5RuE2H3vyqXvHwW2Hlj/UcdivTyrQyrcyOlBmMWJwHIyLpIpIZeA1MBVYCrwCBWV3TgZfd61eAS93MsOF4g/mL1OtGKxORyW585cqgdQJlXQzMdTv8BjBVRHLc4P5UlxZx+dne2a97yuujUbwxxhyxonmpmP7Av92M4gTgb6r6uogsBmaIyDV43V+XAKjqKhGZAawG6oDrVDXwq/194GkgFZjlHgBPAs+JyAagGG8WGqpaLCJ3AYtdvjtVtTgaOzkoxwWYCgswxhjjF7UAo6qbgHEh0vcBU5pZ5x7gnhDphcDYEOlVuAAVYtlTwFNtq3XbDeiVQpxASWUDtfUNJMbbuavGGAN2Jn+HJcbH0T8rhQZg5/6qWFfHGGO6DAswERAYhykqqYxxTYwxpuuwABMB+W4c5rNSCzDGGBNgASYCmlowFTGuiTHGdB0WYCKgsQVjXWTGGNPIAkwEBFow1kVmjDFNLMBEwCAbgzHGmMNYgImAga4Fs6O0ioaG6F3bzRhjjiQWYCIgLSmBrCShpr6BPQerY10dY4zpEizAREhuundvNDsXxhhjPBZgIiQ3zQswNg5jjDEeCzAREmjB2FRlY4zxWICJkKYWjJ1saYwxYAEmYhoDjLVgjDEGsAATMY1dZDYGY4wxgAWYiPG3YLybahpjTM9mASZC0hOFjOQEymvqKa2ojXV1jDEm5izARIiI2DXJjDHGxwJMBAWuqmwnWxpjjAWYiLIWjDHGNLEAE0F2XxhjjGliASaCmlowdrKlMcZYgIkguy+MMcY0sQATQdZFZowxTSzARFDf9GSSEuIoqailoqYu1tUxxpiYsgATQXFxvnNhrBVjjOnhLMBEWCDAFNk4jDGmh7MAE2HWgjHGGI8FmAjLt5lkxhgDWICJuMYuMmvBGGN6OAswEdY0VdlOtjTG9GwWYCLMrkdmjDEeCzARNqBXCnECu8uqqalriHV1jDEmZizARFhifBwDslJQhR37rRVjjOm5LMBEgV0yxhhjOiHAiEi8iCwVkVfd+94iMltE1rvnHF/eW0Vkg4isE5FpvvQCEVnhlj0oIuLSk0XkBZe+UESG+daZ7raxXkSmR3s//QblpAF2sqUxpmfrjBbMDcAa3/tbgDmqOhKY494jIqOBS4ExwLnAIyIS79Z5FLgWGOke57r0a4ASVT0aeAC4z5XVG7gDOBmYBNzhD2TRZidbGmNMlAOMiAwCzgee8CVfCDzjXj8DXORLf15Vq1V1M7ABmCQieUCWqs5XVQWeDVonUNaLwBTXupkGzFbVYlUtAWbTFJSizk62NMYYEO83O0qFi7wI/ArIBH6iql8SkVJVzfblKVHVHBH5A7BAVf/i0p8EZgFbgHtV9Qsu/QzgZlfWSuBcVS1yyzbitVquAlJU9W6XfjtQqaq/DarftXgtI/Ly8gpmzpzZ7n2tqKggLc3rGvt4ZzV3vVfC2NwkfvH53s3ma0uZkchnZVqZVqaV2dEyg02YMGGJqk4IuVBVo/IAvgQ84l5/HnjVvS4Nylfinh8GrvClPwl8DZgIvOVLPwOY6V6vAgb5lm0E+gA3Abf50m8H/rul+hYUFGhHFBYWNr7esLtMh978qp5x39wW87WlzEjkszKtTCvTyuxomcGAQm3mdzWaXWSnAV8WkS3A88DZIvIXYJfr9sI973b5i4DBvvUHAdtd+qAQ6YesIyIJQC+guIWyOkVgDGbH/koaGqLXQjTGmK4sagFGVW9V1UGqOgxv8H6uql4BvAIEZnVNB152r18BLnUzw4bjDeYvUtUdQJmITHbjK1cGrRMo62K3DQXeAKaKSI4b3J/q0jpFSmI8fTOSqK1XdpdVd9ZmjTGmS0mIwTbvBWaIyDXAVuASAFVdJSIzgNVAHXCdqta7db4PPA2k4o3LzHLpTwLPicgGvJbLpa6sYhG5C1js8t2pqsXR3jG//OxU9h6soaikggG9Ujpz08YY0yV0SoBR1XnAPPd6HzClmXz3APeESC8ExoZIr8IFqBDLngKeam+dOyo/J5VlRfv5rLSS0KNfxhjTvdmZ/FFil+03xvR0FmCixK6qbIzp6SzAREm+u1yMnc1vjOmpLMBEibVgjDE9nQWYKPFfUVmjeLUEY4zpqizAREmv1EQyUxKorK2npKI21tUxxphOZwEmiuyqysaYnswCTBQNaryqckWMa2KMMZ3PAkwU2bkwxpiezAJMFNl9YYwxPZkFmCjKz7ZzYYwxPZcFmCgKtGCsi8wY0xNZgIkiO9nSGNOTWYCJor4ZSSQnxLG/spaD1XWxro4xxnQqCzBRJCJ2LowxpseyABNl+XYujDGmh7IAE2XWgjHG9FQWYKKs8WRLG+g3xvQwFmCibFBva8EYY3omCzBR1niypbVgjDE9jAWYKPPfF8YYY3oSCzBR1j8zmfg4YXdZNdV19bGujjHGdBoLMFGWEB/HgKwUAHaUVsW4NsYY03kswHQCu6qyMaYnsgDTCQbZuTDGmB7IAkwnaLqqsp3Nb4zpOSzAdAI72dIY0xNZgOkENlXZGNMTWYDpBHZfGGNMT2QBphMMdAFm5/4q6lVjXBtjjOkcFmA6QUpiPH0zkqlrUEoqG2JdHWOM6RQWYDrJIDcOs6fCzuY3xvQMFmA6SWCgf0+5BRhjTM9gAaaTBE62tBaMMaaniFqAEZEUEVkkIstEZJWI/MKl9xaR2SKy3j3n+Na5VUQ2iMg6EZnmSy8QkRVu2YMiIi49WURecOkLRWSYb53pbhvrRWR6tPYzXPnWRWaM6WGi2YKpBs5W1XHAeOBcEZkM3ALMUdWRwBz3HhEZDVwKjAHOBR4RkXhX1qPAtcBI9zjXpV8DlKjq0cADwH2urN7AHcDJwCTgDn8gi4XAVGXrIjPG9BRtDjAikiMiJ7SWTz0H3dtE91DgQuAZl/4McJF7fSHwvKpWq+pmYAMwSUTygCxVna+qCjwbtE6grBeBKa51Mw2YrarFqloCzKYpKMWEtWCMMT1NWAFGROaJSJZrGSwD/iwivwtjvXgR+RjYjfeDvxDor6o7ANxzP5c9H9jmW73IpeW718Hph6yjqnXAfqBPC2XFTL5vDEbtXBhjTA8g4fzYichSVT1RRL4DDFbVO0Rkuaq22pJx62cD/wZ+BLyvqtm+ZSWqmiMiDwPzVfUvLv1J4DVgK/ArVf2CSz8D+KmqXiAiq4Bpqlrklm3E6xK7GkhW1btd+u1AhareH1Sva/G63sjLyyuYOXNmOLsTUkVFBWlpaS3mufKlXZTXKk9dkEuvlPgW84ZbZlvyWZlWppVpZXa0zGATJkxYoqoTQi5U1VYfwAogD3gTmOjSloezrq+MO4CfAOuAPJeWB6xzr28FbvXlfwM4xeVZ60u/DHjMn8e9TgD2AuLP45Y9BlzWUv0KCgq0IwoLC1vNc9HD7+vQm1/V11fuiFiZbclnZVqZVqaV2dEygwGF2szvarhjML9wP+YbVHWxiBwFrG9pBRHJdS0XRCQV+AKwFngFCMzqmg687F6/AlzqZoYNxxvMX6ReN1qZiEx24ytXBq0TKOtiYK7b4TeAqW68KAeY6tJi6pzR/QGYtWJHjGtijDHRlxBmvh3q6w5T1U1hjMHkAc+4mWBxwAxVfVVE5gMzROQavO6vS1yZq0RkBrAaqAOuU9XAiPj3gaeBVGCWewA8CTwnIhuAYrxZaKhqsYjcBSx2+e5U1eIw9zVqzj8+j1+/vo631uymqraelMTWu8mMMeZIFW6AeQg4KYy0Rqq6HDgxRPo+YEoz69wD3BMivRAYGyK9ChegQix7CniqufrFwtA+6QzPTmBzaR3vrd/b2KIxxpjuqMUAIyKnAKcCuSJyo29RFmCH3+1w6qAUNpce5LUVOyzAGGO6tdbGYJKADLxAlOl7HMAb8zBtNHlQCgBvrd5FdZ2dE2OM6b5abMGo6jvAOyLytKp+2kl16tYGZiZwXF4Wa3Yc4P31e5lynLVijDHdU7izyJJF5HEReVNE5gYeUa1ZN3b+8QMA+I/NJjPGdGPhDvL/A/gj8ARg/ToddN7xefz2zU+Y7brJkhNsOMsY0/2EG2DqVPXRqNakBzkqN4NjB2SydmcZH27Yx1nH9mt9JWOMOcKE20U2U0R+ICJ57nL7vd11yUw7nXd8HmDdZMaY7ivcADMduAn4EFjiHoXRqlRPEAgwb67aSU1dQ4xrY4wxkRdWF5mqDo92RXqao/tlcEz/TNbtKuODjXs56xjrJjPGdC9hBRgRuTJUuqo+G9nq9CxfPH4A63aV8dryHRZgjDHdTrhdZBN9jzOAnwNfjlKdeozzA91kq3dRW2/dZMaY7iXcLrIf+d+LSC/guajUqAcZ2T+Tkf0yWL/7IB9u3MeZo3JjXSVjjImYNt8y2anAu5y+6aDAYP9ry202mTGmewn3lskzReQV9/gP3k3DXm5tPdO6QIB5Y/VO6yYzxnQr4Z5o+Vvf6zrgU3W3KTYdM6p/BiNy09m4p5wFm/ZxxkjrJjPGdA9htWDcRS/X4l1JOQeoiWalehIRaRzsf81OujTGdCPhdpF9HViEd3OvrwMLRcQu1x8h553guslW7aLOusmMMd1EuF1kPwMmqupuABHJBd4CXoxWxXqSY/pnclRuOpv2lLNgUzGnj+wb6yoZY0yHhTuLLC4QXJx9bVjXtEJEOG+s6yZbad1kxpjuIdwg8bqIvCEiV4nIVcB/gNeiV62ep3E22cqd1k1mjOkWWgwwInK0iJymqjcBjwEnAOOA+cDjnVC/HuO4vEyG901nX3kNizYXx7o6xhjTYa21YP4PKANQ1X+p6o2q+mO81sv/RbtyPYmIcJ7d6dIY0420FmCGqery4ERVLQSGRaVGPdgXxwZmk+2kvkFjXBtjjOmY1gJMSgvLUiNZEQNjBmYxtE8aew9aN5kx5sjXWoBZLCLfDU4UkWvwbjpmIsjrJrOTLo0x3UNrAea/gG+LyDwRud893gG+A9wQ/er1PIGz+met3Em9WjeZMebI1eKJlqq6CzhVRM4Cxrrk/6jq3KjXrIcaMzCLwb1T2VZcyeo9NUyKdYWMMaadwr0fzNvA21Gui6Gpm+yxdzbx83dKuPfDWWSnJtErNZFeaYlkpybSKzWR7LREstO89Mq91YyprSclMT7W1TfGmEbhXirGdKLLJw3hjZU7KSquoKq2gZ21Vew8UNXiOr9bOJszRvblnNH9mXJcf3qnJ3VSbY0xJjQLMF3Q0D7pzLvpLAoLCxl9wnhKK2oprahlf2Ut+ytrGl+XVnrpi9ZvZ2NJHW+u3sWbq3cRJzBhWG+mju7P1NEDGNInLda7ZIzpgSzAdGEiQlpSAmlJCQzMbn5W+JIlNQw8ejRvuQCzYNM+Fm0uZtHmYu7+zxqO6Z/J1DH9OWd0f9QmDhhjOokFmG4ir1cq3zplGN86ZRgHqmqZt24Ps1fvYt7a3azbVca6XWU8NHcDg7IS+F7dp3z1pHzSkuzPb4yJHvuF6YayUhL58riBfHncQGrqGliwaR+zV+/i9VU7KTpQzW0vreTXr6/lGxMHc+Upwxjc27rQjDGRZ5fc7+aSEuL43Khc7rpoLB/cfDY/PrkXJw3J5kBVHX96bzOf+83bfPfZQj7csNe6z4wxEWUtmB4kKSGO04ekcsNXClheVMrTH27h1WU7mL16F7NX72JU/wymnzqMr5yYH+uqGmO6gai1YERksIi8LSJrRGSViNzg0nuLyGwRWe+ec3zr3CoiG0RknYhM86UXiMgKt+xBERGXniwiL7j0hSIyzLfOdLeN9SIyPVr7eaQ6YVA2v/v6eD645WxuPGcU/TKT+WTXQX7275VM/uUcHluyn78u/JRFm4spLq+JdXWNMUegaLZg6oD/VtWPRCQTWCIis4GrgDmqeq+I3ALcAtwsIqOBS4ExwEDgLREZpar1wKPAtcACvFsFnAvMAq4BSlT1aBG5FLgP+IaI9AbuACYA6rb9iqqWRHF/j0i5mclcP2Uk3ztzBLNW7uDpD7ewdGspb26q481NKxvz9UlP4uh+GRzdL4OR/TIY2T+To/tl0C8zOYa1N8Z0ZVELMKq6A9jhXpeJyBogH7gQ+LzL9gwwD7jZpT+vqtXAZhHZAEwSkS1AlqrOBxCRZ4GL8ALMhcDPXVkvAn9wrZtpwGxVLXbrzMYLSn+P1v4e6ZIS4rhwfD4Xjs9neVEp/3hnGRWJ2WzYXcaG3QfZV17Dvs3FLAy6ynOv1EQuPjaFgoIYVdwY02VJZwzsuq6rd/GuZ7ZVVbN9y0pUNUdE/gAsUNW/uPQn8YLIFuBeVf2CSz8DuFlVvyQiK4FzVbXILdsInIzXSkpR1btd+u1Apar+Nqhe1+K1jMjLyyuYOXNmu/exoqKCtLTWZ2OFm68rlamq7KtsYNuBOj47UMe2A3UUucfBWu/7c/X4TM4fmR7TelqZVqaVGf0yg02YMGGJqk4IuVBVo/oAMvAu7f9V9740aHmJe34YuMKX/iTwNWAi8JYv/Qxgpnu9ChjkW7YR6APcBNzmS78dr7uu2XoWFBRoRxQWFkY035FQZkNDg/51wac69OZXdejNr+qz87dEZNttyWtlWplWZueWGQwo1GZ+V6M6TVlEEoF/An9V1X+55F0ikueW5wG7XXoRMNi3+iBgu0sfFCL9kHVEJAHoBRS3UJaJIBHh8pOH8J0TMwG4/aWVvLB4a4xrZYzpKqI5i0zwWiFrVPV3vkWvAIFZXdOBl33pl7qZYcOBkcAi9cZyykRksivzyqB1AmVdDMx1EfUNYKqI5LhZalNdmomCLx6dzm3nHwfALf9awT+XFMW4RsaYriCas8hOA74FrBCRj13a/wD3AjPcXTG3ApcAqOoqEZkBrMabgXadejPIAL4PPI13m+ZZ7gFeAHvOTQgoxpuFhqoWi8hdwGKX7051A/4mOr5zxlHUNSj3zlrLTS8uIyFeuHC8nU9jTE8WzVlk7wPSzOIpzaxzD3BPiPRCmm545k+vwgWoEMueAp4Kt76m47535ghq6xq4f/Yn3DhjGQlxcZx/Ql6sq2WMiRG7VIyJqB9NGcn1Zx9NfYNyw/NLeWPVzlhXyRgTIxZgTMT9+JxRfO/MEdQ1KD/820fMXbsr1lUyxsSABRgTcSLCzecewzWnD6e2Xvnecx/xzid7Yl0tY0wnswBjokJEuO3847jylKHU1Ddw7bOFrNhdHetqGWM6kQUYEzUiws8vGMNlk4ZQXdfAL98vse4yY3oQCzAmquLihHsuGsulEwdTUw/ffXYJMxZvi3W1jDGdwAKMibq4OOFXXz2erx2XTn2D8tN/LucPc9fbDc6M6eYswJhOISJcPjaTOy8cgwj89s1P+N+XV1HfYEHGmO7KAozpVFeeMoyHLz+JpPg4nlvwKdf99SOqautbX9EYc8SxAGM63XnH5/HsNZPITEng9VU7ufKpReyvrI11tYwxEWYBxsTE5KP68I/vnUL/rGQWbS7m63+cz479lbGuljEmgizAmJg5dkAW//rBaYzITWfdrjK+9siHrN9VFutqGWMixAKMian87FRe/N6pnDQkm+37q7j4j/Mp3GIXvjamO7AAY2IuJz2Jv35nMl84rj/7K2v55hMLeWldOaUVNbGumjGmAyzAmC4hNSmeP15xEpdNGkx1XQPPLS/j5F/O4eYXl7Pys/2xrp4xph0swJguIyE+jl9+5XienD6Bcf2TqK5r4IXCbXzpoff56iMf8PLHn1FT1xDrahpjwhTNO1oa02YiwpTj+pNd0ZucIcfw3IJPeXFJER9tLeWjrR9zV8ZqLps0hMtPHkJer9RYV9cY0wILMKbLOio3gzsuGMNN047hpaXbeXb+FtbuLOOhuRt4ZN5Gpo7uz6l9ayiIdUWNMSFZgDFdXlpSApefPITLJg1m8ZYSnp2/hddX7mTWyp3MAhbs+4jbvnSctWiM6WIswJgjhogwaXhvJg3vze4DVfxl4VYem7eB/6zYwdvrdnPDlJF8+7ThJCXY0KIxXYH9J5ojUr+sFG48ZxQPntuXL44dQEVNPb+atZbzHnyPDzfujXX1jDFYgDFHuL5p8Tx6RQHPXD2J4X3T2bD7IJf/aSHX/30puw5Uxbp6xvRoFmBMt3DmqFxe/68z+O9zRpGcEMcry7Yz5f53eOK9TdTW29RmY2LBAozpNpIT4vnRlJG8deOZnDO6Pwer67j7P2u44KH3WbTZLj9jTGezAGO6ncG90/jTlRN46qoJDO6dytqdZXz9sfn8Y/XBWFfNmB7FAozpts4+tj+zf3wmN0wZSZzA86sO8reFW2NdLWN6DAswpltLSYznx+eM4u6LjgfgtpdWMGfNrhjXypiewQKM6REuP3kIFx+XToPCD/+2lI+3lca6SsZ0exZgTI9x6ZgMLi4YRGVtPVc/vZgte8tjXSVjujULMKbHEBF+9dXj+dyoXIrLa5j+50XsPVgd62oZ021ZgDE9SmJ8HI988yTG5mfx6b4Krnl6MRU1dbGuljHdkgUY0+NkJCfw1FUTGZSTyrKi/fzob0ups5MxjYk4CzCmR+qXmcIzV08iOy2ROWt3c/vLq1DVWFfLmG7FAozpsUbkZvDk9AkkJ8Tx90Vb+cPcDbGukjHdStQCjIg8JSK7RWSlL623iMwWkfXuOce37FYR2SAi60Rkmi+9QERWuGUPioi49GQRecGlLxSRYb51prttrBeR6dHaR3PkKxjam99feiIicP/sT/hH4bZYV8mYbiOaLZingXOD0m4B5qjqSGCOe4+IjAYuBca4dR4RkXi3zqPAtcBI9wiUeQ1QoqpHAw8A97myegN3ACcDk4A7/IHMmGDnjh3AL748BoBb/7WCj3bYzDJjIiFqAUZV3wWCrzB4IfCMe/0McJEv/XlVrVbVzcAGYJKI5AFZqjpfvQ7yZ4PWCZT1IjDFtW6mAbNVtVhVS4DZHB7ojDnElacM4/+deRR1Dco975dw+n1zuf7vS3n6g80sLyq1KzIb0w6dfUfL/qq6A0BVd4hIP5eeDyzw5StyabXudXB6YJ1trqw6EdkP9PGnh1jHmGbdPO1Y6uqVvy7YQlFJJUUllbyybDsAKYlxnJCfzYlDszlpSA4nDckhNzM5xjU2pmuTaM6cceMir6rqWPe+VFWzfctLVDVHRB4G5qvqX1z6k8BrwFbgV6r6BZd+BvBTVb1ARFYB01S1yC3biNcldjWQrKp3u/TbgQpVvT9E/a7F634jLy+vYObMme3e14qKCtLS0iKWz8qMXZkHy8vZW5vEJ/tqWbevhk/21bL9YP1h+fqlxzMoQxiSnUx+Zjz5WQkMzEwgM+nwjoEjZd+tTCuzrSZMmLBEVSeEXKiqUXsAw4CVvvfrgDz3Og9Y517fCtzqy/cGcIrLs9aXfhnwmD+Pe50A7AXEn8ctewy4rLW6FhQUaEcUFhZGNJ+V2bXKLD5YrXPW7NTfvL5WL3t8vo6+fZYOvfnVkI+T7nxTL3n0Q73ln8v08Xc26pw1O/XVeQu0rr4h6vW0Mq3MaJcZDCjUZn5XO7uL7BVgOnCve37Zl/43EfkdMBBvMH+RqtaLSJmITAYWAlcCDwWVNR+4GJirqioibwC/9A3sT8ULYMa0W056Emcf25+zj+0PQH2DsmH3Qd5csBzN7MfGPQfZuOcgm/aUs6+8hn3lxSzacugQZPLs1xneN52R/TM5OjeDo/t5j2F900hOiA+1WWOOaFELMCLyd+DzQF8RKcKb2XUvMENErsHr/roEQFVXicgMYDVQB1ynqoE+ie/jzUhLBWa5B8CTwHMisgFvMsGlrqxiEbkLWOzy3amqdjtDE1HxccIxAzI5ODiFgoKRjemqys4DVWzcXc6mvQfZuPsgG/YcZM1nJRRXNrB2Zxlrd5YdVtbQ3mmM6JfByH4ZZNVWMXJMLVkpiZ29W8ZEVNQCjKpe1syiKc3kvwe4J0R6ITA2RHoVLkCFWPYU8FTYlTUmQkSEvF6p5PVK5fSRfRvTlyxZwqgxJ7BxTznrd32MNy8AABvLSURBVJWxYY8XfNbvPsjW4go27S1n095yZq/27lXz6w/fZGx+L045qg+TR/Rh4rDeZCR3doeDMR1j31hjOklmSiLjB2czfnD2IelVtfVs3lvO+t0HWbvjAHNWbGVjSR3Li/azvGg/j727ifg44YRBXsA5ZUQfJgztHaO9MCZ8FmCMibGUxHiOy8viuLwsvjxuIFNyyzl27DgKPy1hwaZ9zN+4jxWf7Wfp1lKWbi3lkXkbSYwXRmQncM6+dZxyVB9OGppDSqKN45iuxQKMMV1QenICZ47K5cxRuQCUVdVSuKWE+S7grNy+n7X7alk7dwMPzd1AUnwc44dke11qR/XhxCHZFnBMzFmAMeYIkJmSyFnH9uOsY71zk/dX1vL3txaxR3KYv3Efa3YeYNHmYhZtLub3c9aTlBBHwZAcJrsutbp6u1K06XwWYIw5AvVKTWTiwBQKCkYDUFpRw8LNxczfuI8Fm/axdmeZ19rZtI8H3oJ4gWMXvMe4wdmMH5TNuMHZHN0vg/g4ifGemO7MAowx3UB2WhLTxgxg2pgBABSX17Bos9edtnBzMZ/sLGPV9gOs2n6Avy3cCkBaUjxj83sxfnA24wZlM25wL7snjokoCzDGdEO905M4d2we547NA+D9hYtJ6jeCZdtK+biolGXbSikqqWzsVgtIEEh4eRbxIsTHNT3ipOk5IV5Iio9jRGYD9C3hpCHZuLtoGHMICzDG9ACpCXEUDO/NpOFN05v3HqxmeVEpy7btZ5kLOiUVtdTVhnfl6PW74fVHP2RI7zQuGj+QC0/MZ0RuRrR2wRyBLMAY00P1zUg+5PI3qsqCxUsYN3489Q3a9FCloQH37KWVVNTw57c+ZuHOerYWV/Dg3A08OHcDJwzqxUXj87lg3EC72rSxAGOM8YgISfFCWlLrPwvDSKdhXBYPXHUSCzbt46WlnzFr5c7Gk0Pv/s9qTh+Zy0XjB5IbZovIdD8WYIwx7RYfJ5x2dF9OO7ovd100ljlrdvPvpZ8xb91u3v1kD+9+sgeAgW/PYUS/DEbkZjAiN9177pdBv8xkG7/pxizAGGMiIiUxnvNPyOP8E/IoKa/hPyt28PLHn7H00xK2769i+/4q3lu/95B1MpITGgPO8L7pHNhXwda4ItKTEshITiDdPbzX8aQnJRBnU6uPGBZgjDERl5OexBWTh3LF5KEsWlxIv+HHNd7SYOPucjbu8a4yXVpRy7Ki/Swr2t+08kfLWiw7PSmetARl+OL5DMxOYWB2KgOzU8l3zwOzU8i0K1F3CRZgjDFRFR8nDOubzrC+6Uw5rv8hy4rLa1zQOciWfRVs2rad1KwcyqvrOFhdR3l1ve91HeU19e4Be7Y0fxeOzJQE8rNTyeuVQk1FGfmbl5GaGE9KUjypie6RFE9KYtP7HXtq6L23nNzMZNKT4q3rLgIswBhjYqZ3ehK903szcZg3fXrJknIKCk5sNn9Dg1JeU8e7Cz8iZ9AItpdWsb20ku2llXzmHttLKymrqjv03jvbisKr0Lx5AKQmxpObmew9MpKbXmcm0zcjmV17a8jYWUZmSgJZqYkWkJphAcYYc8SIixMyUxIZkJFAwYi+IfOoKiUVtWwvrWTH/ipWrVvPgPwhVNbWU1lbT1VNfePrypoGqmrrqaipY/veUio1kd1lVVTWetOvtxZXNF+Zt99tqpd414vLSk0gM9l7zkpJJL6mjE+liHGDsxneJ73HjR9ZgDHGdCsi4lpGSYzN70XvyiIKCoa0ut6SJUsoKChAVSmvqWdPWTV7yqrZXVbV+HpPWTV7DlazY28pDfHJHKiqpayqjoqaevZX1rK/shaoPKTcWRu8MaXMlITGS/KcMMi7L1D/rJRofARdhgUYY4zxEREy3My14X3TQ+YJBKOA2voGDlbVNQacA5W1HKiq5f1ln7C3IYNlRaXs2F/F+xv28v6Gppl0A7JSGDe4F8fn96J4dzkrqjYTFyeICALEiSDitZACaZ9tq2Rfyk7SkhJITYonPTmetETvdZobY+oqLSULMMYY00GJ8XHkpCeRk550SHpu9fbGQLTrQBXLtpW6y/J4l+fZeaCKnauqeGOVd6tsPl4d3gYXLmlxcWpiPOnJCfRKrGfUmiUMykllcO807zknjfyc1LBOqO0oCzDGGNMJ+melMHXMAKa6K143NCib95WzbFspa3Yc4LMdu+ibm4sqNKjSoADeZXoaVFG851179pGSnkVFTT0VtfVU1nhddN6jjqrahsYxpr3AxpKdIevTJz2JQS7oJFaXMXJMLVkRnt5tAcYYY2IgLk7clQ28C4QuWVJJQcHYVtcL7p4L1tCgVNbWU1ZVx9yFS0nvN4Sikkr3qKCopJLPSirZV17DvvIalm0rBeDe+LjI7JiPBRhjjOlG4uKk8QoIx/RJomB8/mF5GhqU3WXVjQFn6ZqNUbnFtgUYY4zpYeLihAG9UhjQK4UJw2BwQ+hutA5vJyqlGmOM6fEswBhjjIkKCzDGGGOiwgKMMcaYqLAAY4wxJioswBhjjIkKCzDGGGOiQlQ11nXoEkRkD/BpB4roC+xtNVf4+axMK9PKtDK7YpnBhqpqbsglqmqPCDyAwkjmszKtTCvTyuyKZbblYV1kxhhjosICjDHGmKiwABM5j0c4n5VpZVqZVmZXLDNsNshvjDEmKqwFY4wxJioswBhjjIkKCzDGGGOiwgJMFyAiz7nnG6JUfo6ITBKRzwUe0dhOR4hIcjhp7Sj3sM/UnyYi8SLyl45up5ltR2WfujIRiRORr0e4zPvc8yWRLLcN2xcRGdyG/KeFk9bGOvTuyPqxYoP87SQi/YFfAgNV9YsiMho4RVWfbCbvRPd2karuDlq+Gvgi8ArweUD8y1W1uAP1/A5wAzAI+BiYDMxX1bOD8l0Zan1Vfba923blngZ8rKrlInIFcBLwe1X9NCjfR6p6UmtpvmWnAsPw3ZU1VF2bKXepqp7oe/8GcIGq1oSxPzcAfwbKgCeAE4FbVPXNMLcdKi0Z+FqI/bkzRJlhfe/aWM9RwKNAf1UdKyInAF9W1bvbue13VTXsg5jW/pYisgLve7Owue9DO/fn18DdQCXwOjAO+C9VPeyAQ0SWqGpBmPsT9ndZRIYCI1X1LRFJBRJUtSxEvvV4/79/BmZpMz/cIpILfJfDP8+rg/JlA1eGyHd9OPsYLrtlcvs9jffH/pl7/wnwAhD8z/Z14DfAPLzA8ZCI3KSqL/qy/RHvC34UsMS/OqAuPVBemUsLSVWzgpJuwAtuC1T1LBE5FvhFiFUn+l6nAFOAj4BDfrRF5H1VPT1EPcTb/GHbfxQYJyLjgJ/ifT7PAme68gYA+UCqiJxIU3DNAtJC7aNr8Y3A+4erD+y6v64ichlwOTBcRF7xrZ4J7AsqcgvwgctXHkhU1d+F2PzVqvp7EZkG5ALfxvseNP5wt2OfXgb24/3tq0Pts8/ThPG9C6eePn8CbgIeA1DV5SLyN7wf3/Zse7aI/MQt83+ehx0ohfO3xPvf2Auki8gB/+qE/s6Fuz9TVfWnIvIVoAi4BHgbCNWiXSAiE1V1cYhlgX05BTgVyBWRG32LsoDDbngvIt8FrgV6430Gg/B+C6aEKH4U8AXgarzfkBeAp1X1k6B8LwPvAW/R9HmG8hqwAFgBNLSQr0MswLRfX1WdISK3AqhqnYiE+oP+DJgYaLW4I4y3gMYAo6oPAg+KyKN4X7DA0d+7qrrMX5iqZrpy7gR2As/h/aN9E+/HM1iVqlaJCCKSrKprReSY4Eyq+iP/exHp5coOzne6vx5hqFNVFZEL8VouT4rIdN/yacBVeP9c/h/0MuB/milzAjC6uaM450NgB941lu4PKnd5UN7t7hFH6M/QLxAszgP+rKrLRESC8rR1nwap6rmtbDcg3O9dOPUMSFPVRUGL6zqw7cDR8nW+tEMOlHxa/Vuq6k3ATSLysqpe2Fw+n3D3J9E9nwf8XVWLm/+IOAv4nohswQuageB2gi9PEpCB97vq/x4dAC4OUeZ1wCRgIV5h60WkX6iNu89nNl7wPgsvCP5ARJbhtUznu6xpqnpzczvhk6KqN7aerWMswLRfuYj0wR3Fi8hkvKPQYHFBXWL7aH7say3eF+dfeF/g50TkT6r6UIi801T1ZN/7R0VkIfDroHxFrjn8Et6XswTvx7Q1FcDIMPK1psz9IF0BfE5E4mn6x0ZVnwGeEZGvqeo/wyxzJTAAL4CE5LrgPgVOaa0wVf0FgIhkem/1YAvZl4jIm8Bw4Fa3ziFHgO3Ypw9F5HhVXRFG3nC/d63W02eviIzwlXkxoT/bsLatqsPD2I+AVv+WvnLDCS4Q/v7MFJG1eF1kP3AHf1XNlPlFIAc4w71/FygNqt87wDsi8nRwF3AzqlW1JhDURCQhUOdg7nO/Aq9bayfwI7wu9fHAP/D+zgCvish5qvpaK9t+zrWgXsXXau5Id3zIetsYTPuIyEnAQ8BYvH+SXOBiVV0elO/XeH27f3dJ3wCWhzrKEJHleH3a5e59Ot54yQkh8n4IPAw8j/elvAy4TlVPbaHOZwK9gNeDxxtEZCZNX+544Dhghqre0tLn0BrXXXQ5sFhV3xORIcDnmxkvOR8Yg9dFBzQ7DvE23j/WIg795/iyL0/YXXkiMhavtRYYSN0LXKmqq0JsO85te5Oqlrp//Pzgv7vLmw38L00t0neAO1V1f1C+1XjBfJPbn1BHx4G8ge/dGGAVzX/vAvVMBJLxWnL5oQ5WROQovDO5TwVKgM3AN4N/JNvwnW91PM/3fcuk7X9L8T8Hd5E1sz9XqOqWEPueAxxQ1XoRSQOyVHVniHw3AN+h6eDvIiDkwZ/7fh72w6qHj3v+Gi9IXYkXMH4ArFbVnwWvKyKf4H1Hn1LVz4KW3ayqgYkQZUA63mdZ28JndB1wj9t+oK6qqqFame1mAaYD3BHHMXh/xHWqWhsiz314TeDTXb53gcnNBJgVeN1pVe59Ct4P8/Eh8g4Dfg+chvcF+QBvgHJLO/flTN/bOuBTVS1qT1nt3P4f8cYnzsIblL4Yb0LENSHynhmcBo1HkO3Z9ofAz1T1bff+88Av/cFaRI513YshB5lV9aMQ5f4T74f4GZf0LWCcqn41KN9QQhwdhzoKdt+JH+J1w5UB84GHAt8ZX76wJne4vPHuBzYdr8UdapA5zpWxiNa/8/4f3cbxPFW92Jcn5N8woL1/y6B6NLs/vjxjgdEcelAT6uCnLQd//skAKXgTOOpU9adB+eKAa4CpeJ/nG8AToboLRWQiXvfqUA4dlA+1/d54Byz+fXonKM9G4GRVbe8l+sNiAaYDJIyZTBJ6RsnyZr4YNwLTgX+7pIvwBvL+L8JVD0lame3WxrLaNBkg8Jn4njOAf6nq1PbWoQ11Xaaq41pKE5HHVfVad3QaTJv54f5YVceHkdaWo+MZeH36f3VJlwE5qnpJUL4VNE3uGC9ucoeqfiNEmVvxBtJfAOaG+oFz+earaqtdjiHW6wU852+V+JbdF3ywFSotzO20OKagQZM2ROQOvFmbo/EGvb8IvO8PhL68YR/8NVO3d1S1xaDayvrrgJ/gHbA0dnWGaGWGOrD4UFWnBOV7BbhUVSvaW6dw2BhMO0krs19E5Pt4Td6j3NFPQCZea+Mwqvo7EZlHU2vn26q6tJnthzUdsQ37E85st7Bp2ycDVLrnChEZiDdWdUhffluDVhtsEpHbaZrUcAVet0ojVb3WPZ/VhnIrReR0VX3f1f80mvbT7xq8Vm3g6Pg+XMskRN5jgoLh2+IN9AYLa3JHoEzgArxB5ydF5FXg+UC9fd4Uka/hBf62HJm2NJ53DhAcTL4YIi0c4X7XAi7G675eqqrfdgdYTzST98/AQhHxH/wddkoCNLYgAuLwJjIMCJFvBYd3pe0HCoG7VdU/23GPqs5sZX8g/Fmj9cDH7oDJ3zVp05S7iNZmv/wNmAX8CvCPY5RpCwNprqvlsO6WEMKdjhiuVme7RdmrbsziN3j7rwT9s7cjaLVIRJ5T1W/hfY7DaGpBvIM3rbe59cI6Bwf4Pt5gfy/3vgSvhXpYkRz6N6x3aaEsFZHJqrrA1eVkQh+whD25Q1UrgRnADDcm8Xu8zyB4au2NeP37dSJSRfOt0ZDjeUF52nwA1hp1kzXaoEpVG0SkTkSygN2EnunWpoM/vOnmgf2vw5sGf1hXL97vQz3ebwXApe75AN6U8At8ee8QkSeAORwaEP4VYp/CObB4yT2iygJM+7U4+0W9gdz9eF0Y0RDudMRwtWW2W8Sp6l3u5T/dEXSKBg2GR0GBG/+Yjjf2Exg8hmZ+4FtruQZZgzerbwSQjfd9uIjDp0m3enTsO9pNBK503VqK1ye/OnjDqvoV9/Ln7ii1F143WEhuTOQbeK2HxcBhZ+Oramao/v0Qfut73dx4XrsOwMIhIs8AN6hqqXufA9wfonW/2AXhP+EFhYN4Y0whteHgbzRe8Dwd72/0Hl6rJNhpquo/w3+FiHygqqeJd1Ky37eBY/H+/oEuMsU7KPIL68BCVZ8RkSS882ugmfG0jrIxmDaSNsx+iXI97sbrW21tOmK45YU92y1a2tAyiNT2rsdrZRwF+GfmBI7MDzuaFZE1tH4OTiDv63izdD7C10JR1ftD5D0J30SQ4KNjFwibFWpCQLhEZDNewJwBvBLoqguRL6z+fZe3tatXZKnqAWnmEigdCTISdKWGFtKew5tQ8R7e9OQsDTEbsB3bD3ecbBlwraoudO8n4Y29jQuur4isCHe8x7dOS7NGP483+WQL3nduMDBdVd9tyzZarYMFmLZxfzQB7sM7M71xEXCfHnpuSjTrEdZ0xDaUdz2wDW8mU+BH7t8trxU5zbUMIt0n3My2H1XV74eZ9x/A9ara6nkbIrJSVcd2uIJRFvixDyNfWBMHQoznnQEcMp4nIq+q6pdccAtMOQ4IGdzbsD/L8KbCl7j3vYF3gn+gReRsvKB+Bt5Bxsd43/vft3fbge1rK5NGXNpE4Cm8kzMFLyh9B2/6+fmqOsOX90/AA6p6WGu1nXVcAlyuquvc+1F4J5uGdTmccFkXWRupm+4nIol6+NS/1E6sR7jdFeHqB1yPd7T9FN6Uyc4Uztn5URFOcAlqua4WkXBarm05gTKWasQ7LyL4HKTgLqVw+/fDuXrFl9zL93GtCFVdG6H9uR/vsw9s7xK8cz4OoapzReQdvKB5FvA9vM+gQwGGMMfJ1LvszPFujE4CXXrOjKDspwPTXUBu8VypMCUGgouryyciktjSCu1hAaaNojE42c56hOyuIPR1jFqlqre5mVRT8fp7/+Ca+k+q6sbI1LpFYZ/RHSO/panlepEvPZAWyunAVRH8UYiW5/CuIjENuBPvskNrQuQLd+JAW8bz/oz3OT0k3gmSS/GCTbt/5FX1WREpBM7G+8y/GurIX0Tm4PUCzMfrJpuoHZua36ZxMrdO48nF4s7o1xAnFwPhXkooXIUi8iRNMye/yaHXQYwI6yJrI3e0kUMUBifbWI+wz3NoY7nj8ALMuXgX/psMzNagk8QipauMaYVL2nZeU8hxk46Ml0RDoL9fms5BSgTe0BDn9vjWaal/v03jeeJdPsjfiqhU1WPbsR9tGtcRkQeAArzv2wd4Lan56s2qa7O2jpNJG04ujjTxruB9HYeeAP6IqrZ2sdW2bccCzJFJRBar6kQR+RjvjNxqCXESXxvKux5vNtVevC/7S6paK97ZxutVdUTkan/IdrvEmFZr/C1XwN+iywQ+UNXgWT9HDBFZpKqTRORdvH3cifdD165xkLaM54VoRbzf3lZEiHGdxkW0MK4j3km938Y7kXGAqnbKPXskRicXu4D+TGd8Z62L7MjV3otYNqcvXlfCIUdZ6p0n8KVm1umwrjKmFYaoTavtAh4XbyrvbXgXUMwAbu9AeW0Zz1uO14oYizeNu1S8Kwa0uRURGNfRMC+2KSI/xAuCBXgXRn0KL8h1llZPLo4G9S4LlCsiScGtz0izFkw30FJ3RVfXnVsGRwo59IZngYFebWYsINwyhabxvAl4g9bNjudFshUhInP08EujhEq7Ca9raImqhrqcf1S5Mc+H8MaKHnbJT6hqR4J7uNt+DO8mbuHcA6ndrAXTDQQf+R9hunPL4EjRlhuehUVVVUR24nW31eGNW74oIoeM50WyFSHe9cHSgL6uRea/0dvAEHX8TXu2E0G/xTsP6wyauggfjeYGpenqFd8AHiC8eyC1f3vWgjGmZ4v0+TptGc+LZCtCvIuG/hdeMPmMpgBzAO8Exj90pPxIc7M0y2i6g+ZlQLaqHnYVhQhuM3B79pl4F/o8RKQP6izAGNPDicjjeJf8j8j5OuLdbfXJULPlROQ4VQ01BToi3AD2/2jTpYe6rHBPyIzwNgNXrxjOoWO2LU6EaPf2LMAY0zP5zttIIMwbnh0JpJ23FehsIvI08MegEzKnq+oPOmHbYV+9okPbsQBjTM/U1vM2jhQi8gu82Wltva1ApxLvunbHAFtd0hC8E1wbOIIDvJ8FGGNMtyJN1+mrw7uIZUfvFxQV3TXA+9ksMmNMt6KRv05fVHSHANIaCzDGmG4l0tfpM+3XaTeUMsaYThK4bfCn6t3i+kS8KdOmk1mAMcZ0N1WqWgU03lYAbzDddDLrIjPGdDeRvk6faSebRWaM6baO5Ov0dQcWYIwxxkSFjcEYY4yJCgswxhhjosICjDFRICI/E5FVIrJcRD5215mK1rbmiciEaJVvTHvZLDJjIkxETgG+BJzkbmXdF0iKcbWM6XTWgjEm8vKAvapaDaCqe1V1u4j8r4gsFpGVIvK4u+tjoAXygIi8KyJrRGSiiPxLRNaLyN0uzzARWSsiz7hW0Ysikha8YRGZKiLzReQjEfmHu1MkInKviKx26/62Ez8L04NZgDEm8t4EBovIJyLyiJsqC/AHVZ3obu6VitfKCahR1c8Bf8S7w+R1ePepv0pE+rg8xwCPu6vsHsC71XQj11K6DfiCqp4EFAI3uutyfQUY49a9Owr7bMxhLMAYE2GqehDvFsDXAnuAF0TkKuAsEVno7sNyNjDGt9or7nkFsEpVd7gW0CZgsFu2TVU/cK//ApwetOnJwGjgAxH5GO+ukkPxglEV8ISIfBWoiNjOGtMCG4MxJgpUtR6YB8xzAeX/AScAE1R1m4j8nEOv9Fvtnht8rwPvA/+nwSetBb8XYLaqXhZcHxGZhHexx0uBH+IFOGOiylowxkSYiBwjIiN9SeOBde71XjcucnE7ih7iJhCAd//294OWLwBOE5GjXT3SRGSU214vVX0N757149uxbWPazFowxkReBvCQux5WHbABr7usFK8LbAuwuB3lrgGmi8hjwHrgUf9CVd3juuL+LiLJLvk2oAx4WURS8Fo5P27Hto1pM7tUjDFHABEZBrzqJggYc0SwLjJjjDFRYS0YY4wxUWEtGGOMMVFhAcYYY0xUWIAxxhgTFRZgjDHGRIUFGGOMMVFhAcYYY0xU/H+Jx5PVTwrmQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "fdist.plot(30, cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'those', \"weren't\", 'all', 'because', 'himself', 'while', 'any', 'such', 'his', 'll', 'you', 'has', \"doesn't\", 'how', 'a', \"wasn't\", 'does', \"shouldn't\", 'off', \"didn't\", \"it's\", \"you'll\", 'we', 'as', 'then', 'with', 'didn', 'had', 'wouldn', 'were', 'haven', 'in', 'until', 'mightn', 'couldn', \"shan't\", \"you're\", 'yourself', 'ours', 'under', 'themselves', 'd', \"aren't\", \"hadn't\", 'y', 'not', 'ain', 'from', 'over', 'myself', 'do', 'which', 'further', 'hers', 'their', 'theirs', 'hadn', 'won', 'whom', 'herself', 'during', \"isn't\", 'for', 'ourselves', \"you'd\", 'there', \"don't\", \"couldn't\", 'yours', 'it', 'mustn', 'that', 'most', 'this', 'its', 'wasn', 'been', 'same', 'being', 'here', \"you've\", 'at', 'our', 'once', 'where', 'is', 'more', 's', 'who', 'or', 'few', 'by', 'out', 'my', 'both', 'about', 'isn', 'can', 'now', 'will', 'into', \"mustn't\", 'was', 'doing', 'an', \"wouldn't\", 'between', 'why', 'are', 'against', 'aren', 'just', 'above', 'than', 'up', 'through', 'them', 'itself', 'did', 'ma', 'her', 'on', 're', 'be', \"she's\", \"mightn't\", 'have', 'after', \"needn't\", 'before', 'so', 've', 'shouldn', \"haven't\", 'but', 'yourselves', 'of', 'only', 'o', \"that'll\", 'she', 'very', 'm', \"hasn't\", 'needn', 't', 'and', 'what', 'to', 'the', 'he', \"should've\", 'doesn', 'they', 'if', 'am', 'when', 'don', 'again', \"won't\", 'these', 'your', 'down', 'having', 'each', 'no', 'him', 'nor', 'weren', 'i', 'too', 'own', 'other', 'below', 'should', 'shan', 'some', 'me', \"n't\", 'hasn'}\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))\n",
    "stop_words.update({\"n't\", 's', 'm'})   #добавил поскольку word_tokenize выделяет их как отдельное слова\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete all of the stop words from the list of tokens created by nltk word_tokenize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delstopwords(lst):#функция удаляющая в списке стопслова и \"подчищающая их\" перед удалением\n",
    "    set(stopwords.words('english'))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update({\"n't\", 's', 'm', 'y'})   #добавил поскольку word_tokenize выделяет их как отдельное слова\n",
    "    filtered_sentence = [w for w in lst if not w in stop_words] \n",
    "    return filtered_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are probably a lot of words such as 'apple'/'apples', etc whose presence extends our vocabulary a lot. \n",
    "Please, calculate the size of your vocabulary here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nltk_tokenized_lower'] = df.comment_text_lower.apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nltk_tokenized_lower_filt'] = df.nltk_tokenized_lower.apply(\n",
    "    lambda x: [new_clean_token(x[i]) for i in range(len(x))] ### You code here (list of comprehension)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [explanation, why, the, edits, made, under, my...\n",
       "1         [d'aww, , he, matches, this, background, colou...\n",
       "2         [hey, man, , , , really, not, trying, to, edit...\n",
       "3         [, more, , ca, n't, make, any, real, suggestio...\n",
       "4         [you, , sir, , are, my, hero, , any, chance, y...\n",
       "                                ...                        \n",
       "159566    [, , , , , , and, for, the, second, time, of, ...\n",
       "159567    [you, should, be, ashamed, of, yourself, that,...\n",
       "159568    [spitzer, umm, , theres, no, actual, article, ...\n",
       "159569    [and, it, looks, like, it, was, actually, you,...\n",
       "159570    [, and, , , really, do, n't, think, you, under...\n",
       "Name: nltk_tokenized_lower_filt, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nltk_tokenized_lower_filt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the size of your vocab - number of uniq words from all of the texts ниже размер словаря\n",
    "### Your code here\n",
    "lst_t= []\n",
    "for t in df['nltk_tokenized_lower_filt']:\n",
    "    lst_t.append(delstopwords(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_tokens = list(set(flat_nested(lst_t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.bobistheoilguy.com\n",
      "brears\n",
      "heresiarchess\n",
      "indict\n",
      "dynamique\n",
      "socrates\n",
      "alterans\n",
      "split-off\n",
      "dlw\n",
      "technical/scientific\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for t in lst_tokens:\n",
    "    if i==10:\n",
    "        break\n",
    "    i+=1\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211238"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_init = len(lst_tokens)\n",
    "vocab_size_init #попытаемся уменьшить количество слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "socrates\n",
      "split-off\n",
      "carmaker\n",
      "platzner\n",
      "ordinarily\n",
      "destiny.d\n",
      "root-like\n",
      "clinging\n",
      "realtek\n",
      "semi-logic\n"
     ]
    }
   ],
   "source": [
    "lst = del_words(lst_tokens)\n",
    "i = 0\n",
    "for t in lst:\n",
    "    if i==10:\n",
    "        break\n",
    "    i+=1\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after: 38851\n"
     ]
    }
   ],
   "source": [
    "print('after:', len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: fly\n",
      "Stemmed Word: fli\n"
     ]
    }
   ],
   "source": [
    "# Stemming will help us to reduce the numbed of uniq words in our vocabulary by deleting different forms of the same word\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stem = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('f', 'JJ'), ('l', 'NN'), ('y', 'NN'), ('i', 'NN'), ('n', 'VBP'), ('g', 'NN')]\n",
      "['caress', 'fli', 'die', 'mule', 'deni', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'sensat', 'tradit', 'refer', 'colon', 'plot']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(word))\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized','meeting',\n",
    "           'stating', 'siezing', 'itemization', 'sensational',\n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles1 = [stem.stem(plural) for plural in plurals]\n",
    "singles2 = [lem.lemmatize(word, nltk.pos_tag(word))]\n",
    "print(singles1)\n",
    "print(singles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('going', wordnet.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please, apply stemming  and lemmatization to the tokenized words.  \n",
    "##### 1. Apply stemming first - calculate the number of the uniq words after it \n",
    "##### 2. Apply lemmatization and calculate the same\n",
    "##### 3. Compare, analyse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_stemmed_size = 36922\n"
     ]
    }
   ],
   "source": [
    "### Your code here\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(1500)\n",
    "'''def find_v_stemmed(lst_t1):\n",
    "    steemed_lst_t = set()\n",
    "    for t in lst_t1:\n",
    "        steemed_lst_t.add(stem.stem(t))\n",
    "    return steemed_lst_t\n",
    "slt = set()\n",
    "for lst_t1 in lst_t:\n",
    "    slt.update(find_v_stemmed(lst_t1))\n",
    "vocab_size_stemmed = len(slt)\n",
    "print('vocab_size_stemmed = {}'.format(vocab_size_stemmed))\n",
    "''' \n",
    "slt = set([stem.stem(token) for token in lst])\n",
    "vocab_stemmed_size = len(slt) #len(set([stem.stem(token) for token in lstoken])) ### Your code here \n",
    "print('vocab_stemmed_size = {}'.format(vocab_stemmed_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_lemmatized = 38268\n"
     ]
    }
   ],
   "source": [
    "vocab_size_lemmatized = len(set([lem.lemmatize(token, wordnet.VERB) for token in lst])) ### Your code here \n",
    "print('vocab_size_lemmatized = {}'.format(vocab_size_lemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"What is impeachment?\n",
    "Put simply, it's a process that allows senior figures in government to hold other officials (like judges, the president and cabinet members) to account if they're suspected of committing offences while in office.\n",
    "Those offences can include \"treason, bribery or other high crimes and misdemeanours\".\n",
    "After someone is impeached, they then go on trial in the Senate, the upper house of Congress, the members of which will decide whether they are guilty or not. It's a political trial, not a criminal one.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = sample_text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to load the model firstly, once loaded, you can comment the line \n",
    "\n",
    "# ! python -m spacy download en\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-01140fe63a83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tagger\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"parser\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ner\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mspacy_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\haruhi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\haruhi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "spacy_words = [token.text for token in doc]\n",
    "\n",
    "print(f\"Tokenized words: {spacy_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes which spacy token has: \n",
      " ['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', 'ancestors', 'check_flag', 'children', 'cluster', 'conjuncts', 'dep', 'dep_', 'doc', 'ent_id', 'ent_id_', 'ent_iob', 'ent_iob_', 'ent_type', 'ent_type_', 'get_extension', 'has_extension', 'has_vector', 'head', 'i', 'idx', 'is_alpha', 'is_ancestor', 'is_ascii', 'is_bracket', 'is_currency', 'is_digit', 'is_left_punct', 'is_lower', 'is_oov', 'is_punct', 'is_quote', 'is_right_punct', 'is_sent_start', 'is_space', 'is_stop', 'is_title', 'is_upper', 'lang', 'lang_', 'left_edge', 'lefts', 'lemma', 'lemma_', 'lex_id', 'like_email', 'like_num', 'like_url', 'lower', 'lower_', 'n_lefts', 'n_rights', 'nbor', 'norm', 'norm_', 'orth', 'orth_', 'pos', 'pos_', 'prefix', 'prefix_', 'prob', 'rank', 'remove_extension', 'right_edge', 'rights', 'sent', 'sent_start', 'sentiment', 'set_extension', 'shape', 'shape_', 'similarity', 'string', 'subtree', 'suffix', 'suffix_', 'tag', 'tag_', 'text', 'text_with_ws', 'vector', 'vector_norm', 'vocab', 'whitespace_']\n"
     ]
    }
   ],
   "source": [
    "print(\"Attributes which spacy token has: \\n {}\".format([dir(tok) for tok in doc][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words: ['What', 'be', 'impeachment', '?', 'Put', 'simply', ',', '-PRON-', 'have', 'a', 'process', 'that', 'allow', 'senior', 'figure', 'in', 'government', 'to', 'hold', 'other', 'official', '(', 'like', 'judge', ',', 'the', 'president', 'and', 'cabinet', 'member', ')', 'to', 'account', 'if', '-PRON-', 'be', 'suspect', 'of', 'commit', 'offence', 'while', 'in', 'office', '.', 'Those', 'offence', 'can', 'include', '\"', 'treason', ',', 'bribery', 'or', 'other', 'high', 'crime', 'and', 'misdemeanour', '\"', '.', 'After', 'someone', 'be', 'impeach', ',', 'they', 'then', 'go', 'on', 'trial', 'in', 'the', 'Senate', ',', 'the', 'upper', 'house', 'of', 'Congress', ',', 'the', 'member', 'of', 'which', 'will', 'decide', 'whether', 'they', 'be', 'guilty', 'or', 'not', '.', '-PRON-', 'have', 'a', 'political', 'trial', ',', 'not', 'a', 'criminal', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "# We can access lemmas: \n",
    "\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(f\"Lemmatized words: {lemmas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned words: ['impeachment', '?', 'simply', ',', 'process', 'allow', 'senior', 'figure', 'government', 'hold', 'official', '(', 'like', 'judge', ',', 'president', 'cabinet', 'member', ')', 'account', 'suspect', 'commit', 'offence', 'office', '.', 'offence', 'include', '\"', 'treason', ',', 'bribery', 'high', 'crime', 'misdemeanour', '\"', '.', 'impeach', ',', 'trial', 'Senate', ',', 'upper', 'house', 'Congress', ',', 'member', 'decide', 'guilty', '.', 'political', 'trial', ',', 'criminal', '.']\n"
     ]
    }
   ],
   "source": [
    "# We can filter stop words: \n",
    "\n",
    "cleaned_words = [token.lemma_ for token in doc if not token.is_stop]\n",
    "print(f\"Cleaned words: {cleaned_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned words: ['What', 'be', 'impeachment', 'Put', 'simply', '-PRON-', 'have', 'a', 'process', 'that', 'allow', 'senior', 'figure', 'in', 'government', 'to', 'hold', 'other', 'official', 'like', 'judge', 'the', 'president', 'and', 'cabinet', 'member', 'to', 'account', 'if', '-PRON-', 'be', 'suspect', 'of', 'commit', 'offence', 'while', 'in', 'office', 'Those', 'offence', 'can', 'include', 'treason', 'bribery', 'or', 'other', 'high', 'crime', 'and', 'misdemeanour', 'After', 'someone', 'be', 'impeach', 'they', 'then', 'go', 'on', 'trial', 'in', 'the', 'Senate', 'the', 'upper', 'house', 'of', 'Congress', 'the', 'member', 'of', 'which', 'will', 'decide', 'whether', 'they', 'be', 'guilty', 'or', 'not', '-PRON-', 'have', 'a', 'political', 'trial', 'not', 'a', 'criminal', 'one']\n"
     ]
    }
   ],
   "source": [
    "# We can filter punctuation tokens: \n",
    "\n",
    "cleaned_words = [token.lemma_ for token in doc if not token.is_punct]\n",
    "print(f\"Cleaned words: {cleaned_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(100, random_state=13) # fix random_state to make your experiments reproducible "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create columns [spacy_lemmas], [spacy_tokens], [spacy_filtered_stop_words], [spacy_filtered_punct], [spacy_filtered_stop_punct]  \n",
    "In spacy_filtered_stop_punct filter stop words AND punctuation \n",
    "\n",
    "TIP: Use lambda functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['spacy_lemmas'] = \"\"*df_sample.shape[0] ### Your code here for 4 additional columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_lower</th>\n",
       "      <th>comment_text_tokenized_space</th>\n",
       "      <th>comment_text_tokenized_space_cleaned</th>\n",
       "      <th>nltk_tokenized</th>\n",
       "      <th>spacy_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25680</th>\n",
       "      <td>43fb0a70fae5057f</td>\n",
       "      <td>(incorrect, moronic allegations of)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(incorrect, moronic allegations of)</td>\n",
       "      <td>[(incorrect,, moronic, allegations, of)]</td>\n",
       "      <td>[incorrect, moronic, allegations, of]</td>\n",
       "      <td>[(, incorrect, ,, moronic, allegations, of, )]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74202</th>\n",
       "      <td>c688777f515b665a</td>\n",
       "      <td>As the previous article lead already used, dig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>as the previous article lead already used, dig...</td>\n",
       "      <td>[as, the, previous, article, lead, already, us...</td>\n",
       "      <td>[as, the, previous, article, lead, already, us...</td>\n",
       "      <td>[As, the, previous, article, lead, already, us...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87912</th>\n",
       "      <td>eb233cfbf51fd72f</td>\n",
       "      <td>▲ to ? \\n\\n...character encoding issues. Oops....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>▲ to ? \\n\\n...character encoding issues. oops....</td>\n",
       "      <td>[▲, to, ?, ...character, encoding, issues., oo...</td>\n",
       "      <td>[▲, to, , character, encoding, issues, oops, f...</td>\n",
       "      <td>[▲, to, ?, ..., character, encoding, issues, ....</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130308</th>\n",
       "      <td>b920ddbf9499a110</td>\n",
       "      <td>Yes. I know that was what Mikka did. And you k...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes. i know that was what mikka did. and you k...</td>\n",
       "      <td>[yes., i, know, that, was, what, mikka, did., ...</td>\n",
       "      <td>[yes, i, know, that, was, what, mikka, did, an...</td>\n",
       "      <td>[Yes, ., I, know, that, was, what, Mikka, did,...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147189</th>\n",
       "      <td>386b62dc67bcb66c</td>\n",
       "      <td>Son of a bitchSon of a bitch</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>son of a bitchson of a bitch</td>\n",
       "      <td>[son, of, a, bitchson, of, a, bitch]</td>\n",
       "      <td>[son, of, a, bitchson, of, a, bitch]</td>\n",
       "      <td>[Son, of, a, bitchSon, of, a, bitch]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "25680   43fb0a70fae5057f                (incorrect, moronic allegations of)   \n",
       "74202   c688777f515b665a  As the previous article lead already used, dig...   \n",
       "87912   eb233cfbf51fd72f  ▲ to ? \\n\\n...character encoding issues. Oops....   \n",
       "130308  b920ddbf9499a110  Yes. I know that was what Mikka did. And you k...   \n",
       "147189  386b62dc67bcb66c                       Son of a bitchSon of a bitch   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "25680       1             0        0       0       0              0   \n",
       "74202       0             0        0       0       0              0   \n",
       "87912       0             0        0       0       0              0   \n",
       "130308      0             0        0       0       0              0   \n",
       "147189      1             0        1       0       1              0   \n",
       "\n",
       "                                       comment_text_lower  \\\n",
       "25680                 (incorrect, moronic allegations of)   \n",
       "74202   as the previous article lead already used, dig...   \n",
       "87912   ▲ to ? \\n\\n...character encoding issues. oops....   \n",
       "130308  yes. i know that was what mikka did. and you k...   \n",
       "147189                       son of a bitchson of a bitch   \n",
       "\n",
       "                             comment_text_tokenized_space  \\\n",
       "25680            [(incorrect,, moronic, allegations, of)]   \n",
       "74202   [as, the, previous, article, lead, already, us...   \n",
       "87912   [▲, to, ?, ...character, encoding, issues., oo...   \n",
       "130308  [yes., i, know, that, was, what, mikka, did., ...   \n",
       "147189               [son, of, a, bitchson, of, a, bitch]   \n",
       "\n",
       "                     comment_text_tokenized_space_cleaned  \\\n",
       "25680               [incorrect, moronic, allegations, of]   \n",
       "74202   [as, the, previous, article, lead, already, us...   \n",
       "87912   [▲, to, , character, encoding, issues, oops, f...   \n",
       "130308  [yes, i, know, that, was, what, mikka, did, an...   \n",
       "147189               [son, of, a, bitchson, of, a, bitch]   \n",
       "\n",
       "                                           nltk_tokenized spacy_lemmas  \n",
       "25680      [(, incorrect, ,, moronic, allegations, of, )]               \n",
       "74202   [As, the, previous, article, lead, already, us...               \n",
       "87912   [▲, to, ?, ..., character, encoding, issues, ....               \n",
       "130308  [Yes, ., I, know, that, was, what, Mikka, did,...               \n",
       "147189               [Son, of, a, bitchSon, of, a, bitch]               "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Save the results (df and df_sample) in csv file using df.to_csv function. Share your csv files using google drive or email. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud visualizations \n",
    "\n",
    "Create wordclouds for words cleaned from stop words and punctuation using NLTK library - as in previous task. \n",
    "(with spacy it would work slow, so do not apply it to the whole dataset, use only df_sample part) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms visualizations\n",
    "\n",
    "Create histograms of words frequency or counts for tokens cleaned from stop words and punctuation as in previous day task.  \n",
    "Compare the newly created visualizations to the visualisations from the previous day. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word counts plot \n",
    "\n",
    "Complete the plot as we did previously using FreqDict, but make plot larger (see how to set the plots size) and show 50 most common tokens withing the label and 50 most unfrequent. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
