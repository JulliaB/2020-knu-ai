{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "import os\n",
    "from os import chdir\n",
    "chdir(\n",
    "    r'C:\\Users\\laplace-transform\\AppData\\Local\\Programs\\Python\\Python37\\notebooks\\2020-knu-ai-master\\jigsaw-toxic-comment-classification-challenge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note! Some of these models support only multiclass classification, please, while selecting your dataset,  \n",
    "### be sure that for algorithms which does not support multilabel classification you use only examples with only one label. \n",
    "### Examples without a label in any of the provided categories are clean messages, without any toxicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../jigsaw-toxic-comment-classification-challenge/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As one of the methods to make the training simpier, use only examples, assigned to any category vs clean examples.  \n",
    "For example:  \n",
    "- Select only messages with obscene label == 1  \n",
    "- Select all of the \"clean\" messages  \n",
    "Implement a model which can perform a binary classification  - to understand whether your message is obscene or not.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If you want to perform a multilabel classification, please understand the difference between multilabel and multiclass classification and be sure that you are solving the correct task - choose only algorithms applicable for solving this type of problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To work with multiclass task:  \n",
    "You only need to select messages which have only one label assigned: message cannot be assigned to 2 or more categories.  \n",
    "\n",
    "#### To work with multilabel task: \n",
    "You can work with the whole dataset - some of your messages have only 1 label, some more than 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we worked only with words vectorization. But we need to have a vector for each text, not only words from it. \n",
    "\n",
    "Before starting a text vectorization, please, make sure you are working with clean data - use the dataset created on the previous day. Cleaned from punctuation, stop words, lemmatized or stemmed, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(tokenizer, lemmatizer, stop_words, punctuation, text): \n",
    "    tokens = tokenizer(text.lower())\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return [token for token in lemmas if token not in stop_words and token not in punctuation]\n",
    "\n",
    "df['cleaned'] = df.comment_text.apply(lambda x: preprocess_text(word_tokenize, lemmatizer, stop_words, punctuation, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[d'aww, match, background, colour, 'm, seeming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, 'm, really, trying, edit, war, 's, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[``, ca, n't, make, real, suggestion, improvem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page, 's]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                             cleaned  \n",
       "0  [explanation, edits, made, username, hardcore,...  \n",
       "1  [d'aww, match, background, colour, 'm, seeming...  \n",
       "2  [hey, man, 'm, really, trying, edit, war, 's, ...  \n",
       "3  [``, ca, n't, make, real, suggestion, improvem...  \n",
       "4            [sir, hero, chance, remember, page, 's]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_nested(nested):\n",
    "    flatten = []\n",
    "    for item in nested:\n",
    "        if isinstance(item, list):\n",
    "            flatten.extend(item)\n",
    "        else:\n",
    "            flatten.append(item)\n",
    "    return flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(flat_nested(df.cleaned.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249531"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, probably you vocabulary is too large.  \n",
    "Let's try to make it smaller.  \n",
    "For example, let's get rig of words, which has counts in our dataset less than some threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict \n",
    "\n",
    "cnt_vocab = Counter(flat_nested(df.cleaned.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"''\", 241319),\n",
       " ('``', 156982),\n",
       " ('article', 73264),\n",
       " (\"'s\", 66766),\n",
       " (\"n't\", 57144),\n",
       " ('wa', 56590),\n",
       " ('page', 56239),\n",
       " ('wikipedia', 45413),\n",
       " ('talk', 35356),\n",
       " ('ha', 31896)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vocab.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clean words which are shorter that particular length and occur less than N times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_count = 10\n",
    "threshold_len = 4 \n",
    "cleaned_vocab = [token for token, count in cnt_vocab.items() if count > threshold_count and len(token) > threshold_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18705"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!  \n",
    "Let's try to vectorize the text summing one-hot vectors for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = defaultdict()\n",
    "\n",
    "for i, token in enumerate(cleaned_vocab): \n",
    "    empty_vec = np.zeros(len(cleaned_vocab))\n",
    "    empty_vec[i] = 1 \n",
    "    vocabulary[token] = empty_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary['hardcore']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rigth now we have vectors for words (words are one-hot vectorized)  \n",
    "Let's try to create vectors for texts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'fair', 'use', 'rationale', 'image', 'wonju.jpg', 'thanks', 'uploading', 'image', 'wonju.jpg', 'notice', 'image', 'page', 'specifies', 'image', 'used', 'fair', 'use', 'explanation', 'rationale', 'use', 'wikipedia', 'article', 'constitutes', 'fair', 'use', 'addition', 'boilerplate', 'fair', 'use', 'template', 'must', 'also', 'write', 'image', 'description', 'page', 'specific', 'explanation', 'rationale', 'using', 'image', 'article', 'consistent', 'fair', 'use', 'please', 'go', 'image', 'description', 'page', 'edit', 'include', 'fair', 'use', 'rationale', 'uploaded', 'fair', 'use', 'medium', 'consider', 'checking', 'specified', 'fair', 'use', 'rationale', 'page', 'find', 'list', \"'image\", 'page', 'edited', 'clicking', '``', \"''\", 'contribution', \"''\", \"''\", 'link', 'located', 'top', 'wikipedia', 'page', 'logged', 'selecting', '``', \"''\", 'image', \"''\", \"''\", 'dropdown', 'box', 'note', 'fair', 'use', 'image', 'uploaded', '4', 'may', '2006', 'lacking', 'explanation', 'deleted', 'one', 'week', 'uploaded', 'described', 'criterion', 'speedy', 'deletion', 'question', 'please', 'ask', 'medium', 'copyright', 'question', 'page', 'thank', 'talk', '•', 'contribs', '•', 'unspecified', 'source', 'image', 'wonju.jpg', 'thanks', 'uploading', 'image', 'wonju.jpg', 'noticed', 'file', \"'s\", 'description', 'page', 'currently', 'doe', \"n't\", 'specify', 'created', 'content', 'copyright', 'status', 'unclear', 'create', 'file', 'need', 'specify', 'owner', 'copyright', 'obtained', 'website', 'link', 'website', 'wa', 'taken', 'together', 'restatement', 'website', \"'s\", 'term', 'use', 'content', 'usually', 'sufficient', 'information', 'however', 'copyright', 'holder', 'different', 'website', \"'s\", 'publisher', 'copyright', 'also', 'acknowledged', 'well', 'adding', 'source', 'please', 'add', 'proper', 'copyright', 'licensing', 'tag', 'file', 'doe', \"n't\", 'one', 'already', 'created/took', 'picture', 'audio', 'video', 'tag', 'used', 'release', 'gfdl', 'believe', 'medium', 'meet', 'criterion', 'wikipedia', 'fair', 'use', 'use', 'tag', 'one', 'tag', 'listed', 'wikipedia', 'image', 'copyright', 'tag', 'fair', 'use', 'see', 'wikipedia', 'image', 'copyright', 'tag', 'full', 'list', 'copyright', 'tag', 'use', 'uploaded', 'file', 'consider', 'checking', 'specified', 'source', 'tagged', 'find', 'list', 'file', 'uploaded', 'following', 'link', 'unsourced', 'untagged', 'image', 'may', 'deleted', 'one', 'week', 'tagged', 'described', 'criterion', 'speedy', 'deletion', 'image', 'copyrighted', 'non-free', 'license', 'per', 'wikipedia', 'fair', 'use', 'image', 'deleted', '48', 'hour', 'question', 'please', 'ask', 'medium', 'copyright', 'question', 'page', 'thank', 'talk', '•', 'contribs', '•', '``']\n"
     ]
    }
   ],
   "source": [
    "sample_text = df.cleaned[10]\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot vectorization and count vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vector = np.zeros(len(cleaned_vocab))\n",
    "\n",
    "for token in sample_text: \n",
    "    try: \n",
    "        sample_vector += vocabulary[token]\n",
    "    except KeyError: \n",
    "\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we have count vectorization for our text.   \n",
    "Use this pipeline to create vectors for all of the texts. Save them into np.array. i-th raw in np.array is a vector which represents i-th text from the dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def vocabulary_interact_vect(\n",
    "    sample_text:     np.ndarray,\n",
    "    vocabulary_len:  int,\n",
    "    vect_vocabulary: defaultdict,\n",
    ") -> np.ndarray:\n",
    "    \n",
    "    text_vector = np.zeros(len(vect_vocabulary))\n",
    "    \n",
    "    for token in sample_text:\n",
    "        try: \n",
    "            text_vector += vect_vocabulary[token]\n",
    "        except KeyError: \n",
    "            continue\n",
    "    return text_vector\n",
    "\n",
    "\n",
    "# because we're going to use matrix interpret on train/test data\n",
    "# by the way, this algorithm is pretty slow, but gives correct results\n",
    "def vocabulary_interact_sparse(\n",
    "    corpus:          pd.Series,\n",
    "    vocabulary_len:  int,\n",
    "    vect_vocabulary: defaultdict,\n",
    ") -> csr_matrix:\n",
    "    \n",
    "    corpus_len = len(corpus)\n",
    "    \n",
    "    texts_vectorized = csr_matrix((corpus_len, vocabulary_len))\n",
    "    \n",
    "    for j in range(corpus_len):\n",
    "        current_text_vector = vocabulary_interact_vect(\n",
    "            corpus[j], vocabulary_len, vect_vocabulary\n",
    "        )\n",
    "        \n",
    "        current_sparse_matr = csr_matrix(\n",
    "            (current_text_vector,(np.full(N, j),np.arange(N))), \n",
    "            shape = (corpus_len, vocabulary_len)\n",
    "        )\n",
    "        \n",
    "        texts_vectorized += current_sparse_matr\n",
    "    \n",
    "    return texts_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(cleaned_vocab)\n",
    "\n",
    "text_vectorized = vocabulary_interact_sparse(\n",
    "    corpus = pd.Series([df.cleaned[10]]), vocabulary_len = N, vect_vocabulary = vocabulary\n",
    ")\n",
    "\n",
    "text_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_load = True\n",
    "\n",
    "path0 = os.getcwd() + os.sep + 'document_term_matrix.npz'\n",
    "\n",
    "if util_load:\n",
    "    text_vectorized = load_npz(path0)\n",
    "else:\n",
    "    text_vectorized = vocabulary_interact_sparse(\n",
    "        corpus = df.cleaned, vocabulary_len = N, vect_vocabulary = vocabulary\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<159571x18705 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2717363 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "util_save = False\n",
    "\n",
    "if util_save:\n",
    "    save_npz(path0, text_vectorized)\n",
    "    \n",
    "util_del = True\n",
    "\n",
    "if util_del:\n",
    "    del text_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step is to train any classification model on top of the received vectors and report the quality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, select any of the proposed pipelines for performing a text classification task. (Binary, multiclass or multilabel).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main task to calculate our models performance is to create a training and test sets. When you selected a texts for your task, please, use https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html to have at least two sets - train and test.  \n",
    "\n",
    "Train examples you will use to train your model on and test examples to evaluate your model - to understand how your model works on the unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   insult                                            cleaned\n",
      "0       0  [explanation, edits, made, username, hardcore,...\n",
      "1       0  [d'aww, match, background, colour, 'm, seeming...\n",
      "2       0  [hey, man, 'm, really, trying, edit, war, 's, ...\n",
      "3       0  [``, ca, n't, make, real, suggestion, improvem...\n",
      "4       0            [sir, hero, chance, remember, page, 's]\n",
      "\n",
      "        insult                                            cleaned\n",
      "151218       1  [``, previous, conversation, fucking, shit, ea...\n",
      "151219       1                        [mischievious, pubic, hair]\n",
      "151220       1  [absurd, edits, absurd, edits, great, white, s...\n",
      "151221       1  [``, hey, listen, n't, ever, delete, edits, ev...\n",
      "151222       1  ['m, going, keep, posting, stuff, u, deleted, ...\n"
     ]
    }
   ],
   "source": [
    "### Your code here, splitting your dataset into train and test parts.\n",
    "\n",
    "# This time i'm going to handle binary classification task\n",
    "# First of all, I'll divide df into two separate groups:\n",
    "# - non-toxic data\n",
    "# - insult labelled data\n",
    "\n",
    "df_categories = [\n",
    "    'identity_hate', 'insult', 'obscene', 'severe_toxic', 'threat', 'toxic'\n",
    "]\n",
    "\n",
    "crucial_data = df[[df_categories[1], 'cleaned']]\n",
    "\n",
    "df_non_toxic = crucial_data[~df[df_categories].any(axis = 'columns')]\n",
    "df_insulting = crucial_data[df.insult != 0]\n",
    "\n",
    "df_combined = df_non_toxic.append(df_insulting).reset_index(drop = True)\n",
    "\n",
    "print(\n",
    "    df_combined.head(),\n",
    "    df_combined.tail(),\n",
    "    sep = '\\n\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "custom_test_size = 0.25\n",
    "\n",
    "X = df_combined['cleaned']\n",
    "Y = df_combined['insult']\n",
    "\n",
    "# making train and test sets for future model\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size = custom_test_size\n",
    ")\n",
    "\n",
    "X_train = X_train.reset_index(drop = True)\n",
    "X_test = X_test.reset_index(drop = True)\n",
    "\n",
    "Y_train = Y_train.reset_index(drop = True)\n",
    "Y_test = Y_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [recaptcha, 1, n't, defamatory, 2, relates, co...\n",
       "1    [``, would, appear, administrator, dealing, se...\n",
       "2    [``, :i, double, checked, ..., sure, enough, d...\n",
       "3    [ffs, ca, n't, follow, wikipedia, rule, pray, ...\n",
       "4    [``, every, market, ha, protocol, indeed, coul...\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [regarding, edit, wadi-al-ahmar, libya, blog, ...\n",
       "1    [recently, created, page, title, new, page, mo...\n",
       "2             [redirect, talk, battle, fort, tularosa]\n",
       "3    [``, indonesia, '', '', majority, present, day...\n",
       "4    [comment, talk, page, completely, asinine, fuc...\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please, review again this article or read it if you have not done it before. \n",
    "\n",
    "https://medium.com/@paritosh_30025/natural-language-processing-text-data-vectorization-af2520529cf7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement calculating a tf-idf score for each of the words from your vocabulary. \n",
    "\n",
    "The main goal of this task is to create a dictionary - keys of the dictionary would be tokens and values would be corresponding tf-idf score of the token.\n",
    "\n",
    "#### Calculate it MANUALLY and compare the received scores for words with the sklearn implementation:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tip: \n",
    "\n",
    "##### TF = (Number of time the word occurs in the current text) / (Total number of words in the current text)  \n",
    "\n",
    "##### IDF = (Total number of documents / Number of documents with word t in it)\n",
    "\n",
    "##### TF-IDF = TF*IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you calculated a tf-idf score for each of the words in your vocabulary - revectorize the texts.  \n",
    "Instead of using number of occurences of the i-th word in the i-th cell of the text vector, use it's tf-idf score.   \n",
    "\n",
    "Revectorize the documents, save vectors into np.array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here for obtaining a tf-idf vectorized documents. \n",
    "\n",
    "# Like mentioned above, we're going to define \n",
    "# the tf-idf vectorization method manually.\n",
    "\n",
    "def TF_binary(\n",
    "    term:     str,       # our token\n",
    "    document: list       # our text - a list of cleaned tokens\n",
    ") -> bool:\n",
    "    return term in document\n",
    "\n",
    "\n",
    "def TF(\n",
    "    term:     str,       # our token\n",
    "    document: list       # our text - a list of cleaned tokens\n",
    ") -> float:\n",
    "    return document.count(term)/len(document) if document else 0\n",
    "\n",
    "\n",
    "def IDF(\n",
    "    term:     str,       # our token\n",
    "    corpus:   pd.Series, # list of all texts, to which the mentioned one belongs\n",
    "    use_log:  bool       # apply log func on a result or not\n",
    ") -> float:\n",
    "    TF_binary_v_rough = np.vectorize(lambda doc: TF_binary(term, doc))\n",
    "    \n",
    "    return (np.log if use_log else (lambda t: t))(\n",
    "        len(corpus)/len(corpus[TF_binary_v_rough(corpus)])\n",
    "    )\n",
    "\n",
    "\n",
    "def TF_IDF(\n",
    "    term:     str,       # our token\n",
    "    document: list,      # our text - a list of cleaned tokens\n",
    "    corpus:   pd.Series, # list of all texts, to which the mentioned one belongs\n",
    "    use_log:  bool       # use IDF or IDF_enchanced in calculations below\n",
    ") -> float:\n",
    "    return TF(term, document) * (\n",
    "        IDF(term, corpus, use_log)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term:       one\n",
      "TF_bin:     1\n",
      "TF_std:     0.014492753623188406\n",
      "\n",
      "8     [sorry, word, 'nonsense, wa, offensive, anyway...\n",
      "9             [alignment, subject, contrary, dulithgow]\n",
      "10    [``, fair, use, rationale, image, wonju.jpg, t...\n",
      "11             [bbq, man, let, discus, it-maybe, phone]\n",
      "12    [hey, ..., it.., talk, ..., exclusive, group, ...\n",
      "Name: cleaned, dtype: object\n",
      "\n",
      "Term:       one\n",
      "IDF_std:    1.6666666666666667\n",
      "IDF_log:    0.5108256237659907\n",
      "\n",
      "Term:       one\n",
      "TF-IDF_std: 0.024154589371980676\n",
      "TF-IDF_log: 0.007403269909652039\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample = df.cleaned[10]\n",
    "ex_term = 'one'\n",
    "\n",
    "print(\n",
    "    \"Term:{tm:>10}\\n\"\n",
    "    \"TF_bin:{tb:>6}\\n\"\n",
    "    \"TF_std:{ts:>25}\\n\".format(\n",
    "        tm = ex_term,\n",
    "        tb = TF_binary(ex_term, df_sample),\n",
    "        ts = TF(ex_term, df_sample) # 16/276\n",
    "    )\n",
    ")\n",
    "\n",
    "df_sample_series = df.cleaned[8:13]\n",
    "\n",
    "print(df_sample_series, end = '\\n\\n')\n",
    "\n",
    "print(\n",
    "    \"Term:{tm:>10}\\n\"\n",
    "    \"IDF_std:{ib:>22}\\n\"\n",
    "    \"IDF_log:{il:>22}\\n\".format(\n",
    "        tm = ex_term,\n",
    "        ib = IDF(ex_term, df_sample_series, use_log = False),\n",
    "        il = IDF(ex_term, df_sample_series, use_log = True)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Term:{tm:>10}\\n\"\n",
    "    \"TF-IDF_std: {tib}\\n\"\n",
    "    \"TF-IDF_log: {til}\\n\".format(\n",
    "        tm = ex_term,\n",
    "        tib = TF_IDF(ex_term, df_sample, df_sample_series, use_log = False),\n",
    "        til = TF_IDF(ex_term, df_sample, df_sample_series, use_log = True)\n",
    "    )\n",
    ")\n",
    "\n",
    "del df_sample_series, df_sample, ex_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awful. definitely needs proper manual vectorization\n",
    "def TF_IDF_dict(\n",
    "    corpus:     pd.Series,   # list of all texts\n",
    "    vocabulary: defaultdict, # generated set of uniq words\n",
    "    use_log:    bool         # use IDF or IDF_enchanced in calculations below\n",
    ") -> defaultdict:\n",
    "    \n",
    "    tf_idf_dict = defaultdict()\n",
    "    j = 1\n",
    "    for token in vocabulary:\n",
    "        clear_output()\n",
    "        print(\"{c} | {e}\".format(c = j, e = len(vocabulary)))\n",
    "        tf_idf_dict[token] = np.array(\n",
    "            [(doc.count(token)/len(doc) if doc else 0) for doc in corpus]\n",
    "        ) * (np.log if use_log else (lambda t: t))(\n",
    "        len(corpus)/len(corpus[np.vectorize(lambda doc: token in doc)(corpus)])\n",
    "        )\n",
    "        j += 1\n",
    "        \n",
    "        \n",
    "def TF_IDF_sparse(\n",
    "    corpus:     pd.Series,   # list of all texts\n",
    "    vocabulary: defaultdict, # generated set of uniq words\n",
    "    use_log:    bool         # use IDF or IDF_enchanced in calculations below\n",
    ") -> csr_matrix:\n",
    "    \n",
    "    text_vectorized = csr_matrix(len(corpus),len(vocabulary))\n",
    "    \n",
    "    IDF_vectorized = len(corpus) / np.array(\n",
    "        [\n",
    "            np.sum(corpus.apply(lambda l: t in l)) for t in vocabulary\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for TF_IDF_sparse usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<113417x18705 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1967295 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no-sklearn implementation\n",
    "\n",
    "path1 = os.getcwd() + os.sep + 'x_train_tf_idf_sparse.npz'\n",
    "\n",
    "if util_load:\n",
    "    X_train_count_matrix = load_npz(path1)\n",
    "else:\n",
    "    X_train_count_matrix = vocabulary_interact_sparse(\n",
    "        corpus = X_train, vocabulary_len = N, vect_vocabulary = vocabulary\n",
    "    )\n",
    "\n",
    "X_train_count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if util_save:\n",
    "    \n",
    "    save_npz(path1, X_train_count_matrix)\n",
    "    \n",
    "if util_del:\n",
    "    \n",
    "    del X_train_count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn implementation\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X_train_concat = X_train.str.join(' ')\n",
    "X_test_concat = X_test.str.join(' ')\n",
    "\n",
    "pipeline_init = Pipeline(\n",
    "    [\n",
    "        ('cnt-matr', CountVectorizer(vocabulary = cleaned_vocab)),\n",
    "        ('tf-idf-sparse-matr', TfidfTransformer())\n",
    "    ]\n",
    ").fit(X_train_concat)\n",
    "\n",
    "X_train_tf_idf_sparse = pipeline_init.transform(X_train_concat)\n",
    "X_test_tf_idf_sparse = pipeline_init.transform(X_test_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<113417x18705 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1983489 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf_idf_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was said before, select any of the text classification models for the selected task and train the model. \n",
    "\n",
    "When the model is trained, you need to evaluate it somehow. \n",
    "\n",
    "Read about True positive, False positive, False negative and True negative counts and how to calculate them:   \n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative \n",
    "\n",
    "##### Calculate TP, FP, FN and TN on the test set for your model to measure its performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier = LogisticRegression(\n",
    "    random_state = 0                     # <=> seed equiv.\n",
    ").fit(X_train_tf_idf_sparse, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37806,)\n",
      "(37806,)\n"
     ]
    }
   ],
   "source": [
    "#linear_classifier.score(X_train_tf_idf_sparse, Y)\n",
    "\n",
    "Y_prediction = linear_classifier.predict(X_test_tf_idf_sparse)\n",
    "\n",
    "print(Y_prediction.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(957, 35843, 962, 44)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assuming that we have only 0 or 1 in Y_test/Y_pred\n",
    "\n",
    "def TP_FP_FN_TN(\n",
    "    y_test,       # validated labels\n",
    "    y_pred        # predicted labels\n",
    ") -> list:\n",
    "    \n",
    "    TP = lambda y_test, y_pred: np.sum([(y_test==y_pred) & y_test]) \n",
    "    FP = lambda y_test, y_pred: np.sum([(y_test==y_pred) & ~y_test])\n",
    "    \n",
    "    TN = lambda y_test, y_pred: np.sum([(y_test!=y_pred) & y_test])\n",
    "    FN = lambda y_test, y_pred: np.sum([~((y_test==y_pred) | y_test)])\n",
    "    \n",
    "    return list(map(lambda f: f(y_test, y_pred),[TP, FP, TN, FN]))\n",
    "\n",
    "TP, FP, TN, FN = TP_FP_FN_TN(Y_test, Y_prediction)\n",
    "print(Y_prediction.shape == np.sum([TP, FP, TN, FN]))\n",
    "TP, FP, TN, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next step is to calculate  Precision, Recall, F1 and F2 score \n",
    "\n",
    "https://en.wikipedia.org/wiki/Sensitivity_and_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.026005434782608694,\n",
       " 0.9560439560439561,\n",
       " 0.05063358112219253,\n",
       " 0.11726791491030292)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def PRE_REC_F1_F2(\n",
    "    TP: int,       # True positive\n",
    "    FP: int,       # False positive\n",
    "    FN: int        # False negative\n",
    ") -> list: # f\n",
    "    \n",
    "    PRE = lambda tp, fp: tp / (tp + fp)\n",
    "    REC = lambda tp, fn: tp / (tp + fn)\n",
    "    \n",
    "    Fbeta = lambda pre, rec, beta: (\n",
    "        (1 + beta ** 2) * (pre * rec) / (((beta ** 2) * pre) + rec)\n",
    "    )\n",
    "    \n",
    "    PRE_R, REC_R = PRE(TP, FP), REC(TP, FN)\n",
    "    \n",
    "    return [PRE_R, REC_R, Fbeta(PRE_R, REC_R, 1), Fbeta(PRE_R, REC_R, 2)]\n",
    "\n",
    "prec, rec, F1, F2 = PRE_REC_F1_F2(TP, FP, FN)\n",
    "prec, rec, F1, F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining accuracy measure\n",
    "def ACC(\n",
    "    TP: int,       # True positive\n",
    "    FP: int,       # False positive\n",
    "    TN: int,       # True negative\n",
    "    FN: int        # False negative\n",
    "):\n",
    "    return (TP + FP) / (TP + FP + TN + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733904671216209"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A0 = ACC(TP, FP, TN, FN)\n",
    "A0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35843,    44],\n",
       "       [  962,   957]], dtype=int64)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using sklearn package\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test, Y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9560439560439561, 0.4986972381448671, 0.6554794520547945)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(Y_test, Y_prediction, average = 'binary')[:-1]\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733904671216209"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate these metrics for the vectorization created using count vectorizing and for tf-idf vectorization.  \n",
    "Compare them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and improvements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all of the vectorization pipelines we used all of the words, which were available in our dictionary, as experiment try to use the most meaningful words - select them using TF-IDF score. (for example for each text you can select not more than 10 words for vectorization, or less). \n",
    "\n",
    "Compare this approach with the first and second ones. Did your model improve? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another code for task above\n",
    "\n",
    "X_full_concat = df.cleaned.str.join(' ')\n",
    "\n",
    "pipeline_full = Pipeline(\n",
    "    [\n",
    "        ('cnt-matr', CountVectorizer(vocabulary = cleaned_vocab)),\n",
    "        ('tf-idf-sparse-matr', TfidfTransformer())\n",
    "    ]\n",
    ").fit(X_full_concat)\n",
    "\n",
    "X_full_concat_tf_idf = pipeline_full.transform(X_full_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<159571x18705 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2745109 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full_concat_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full_concat_tf_idf[0].nonzero()[1][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_filter = np.zeros(len(cleaned_vocab))\n",
    "\n",
    "M = len(X_full_concat)\n",
    "\n",
    "for j in range(M):\n",
    "    \n",
    "    to_add = np.zeros(len(cleaned_vocab))\n",
    "    \n",
    "    idx_non_zero = X_full_concat_tf_idf[j].nonzero()[1][::-1]\n",
    "    \n",
    "    np.put(to_add, idx_non_zero, np.ones(len(idx_non_zero)))\n",
    "    \n",
    "    to_filter += to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2745109 2745109.0\n"
     ]
    }
   ],
   "source": [
    "print(len(X_full_concat_tf_idf.nonzero()[0]), sum(to_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17409"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_filter[to_filter != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['explanation', 'edits', 'username', ..., 'sampi', 'ragan', 'garan'],\n",
       "      dtype='<U173')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vocabulary = np.array(cleaned_vocab)[np.where(to_filter != 0)]\n",
    "new_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_u, X_test_u, Y_train_u, Y_test_u = train_test_split(\n",
    "    X, Y, test_size = custom_test_size\n",
    ")\n",
    "\n",
    "X_train_u = X_train_u.reset_index(drop = True)\n",
    "X_test_u = X_test_u.reset_index(drop = True)\n",
    "\n",
    "Y_train_u = Y_train_u.reset_index(drop = True)\n",
    "Y_test_u = Y_test_u.reset_index(drop = True)\n",
    "\n",
    "pipeline_upd = Pipeline(\n",
    "    [\n",
    "        ('cnt-matr', CountVectorizer(vocabulary = new_vocabulary)),\n",
    "        ('tf-idf-sparse-matr', TfidfTransformer())\n",
    "    ]\n",
    ").fit(X_train_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_concat_u =  X_train_u.str.join(' ')\n",
    "X_test_concat_u = X_test_u.str.join(' ')\n",
    "\n",
    "X_train_tf_idf_sparse_upd = pipeline_init.transform(X_train_concat_u)\n",
    "X_test_tf_idf_sparse_upd = pipeline_init.transform(X_test_concat_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier.fit(X_train_tf_idf_sparse_upd, Y_train_u)\n",
    "\n",
    "Y_prediction_upd = linear_classifier.predict(X_test_tf_idf_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35849,    38],\n",
       "       [  921,   998]], dtype=int64)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_test, Y_prediction_upd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9633204633204633, 0.5200625325690463, 0.6754653130287647)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(\n",
    "    Y_test, Y_prediction_upd, average = 'binary'\n",
    ")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9226048775326667"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1 = accuracy_score(Y_test_u, Y_prediction_upd)\n",
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1 > A0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic_cls.joblib']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "#from joblib import load\n",
    "\n",
    "dump(linear_classifier, 'logistic_cls.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Висновки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additionally, visualisations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now you have a vector for each word from your vocabulary. \n",
    "You have vectors with lenght > 18000, so the dimension of your space is more than 18000 - it's impossible to visualise it in 2d space. \n",
    "\n",
    "So try to research and look for algorithms which perform dimensionality reduction. (t-SNE, PCA) \n",
    "Try to visualise obtained vectors in a vectorspace, only subset from the vocabulary, don't plot all of the words. (100) \n",
    "\n",
    "Probably on this step you will realise how this type of vectorization using these techniques is not the best way to vectorize words. \n",
    "\n",
    "Please, analyse the obtained results and explain why visualisation looks like this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Here we'll be using princomp method to make visualizations possible\n",
    "# .. later, because having troubles with memory. Now using alternative for sparse matrices\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Singular_value_decomposition\n",
    "# https://arxiv.org/pdf/0909.4061.pdf\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_svd_model = TruncatedSVD(n_components = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_vectors = np.array(list(vocabulary.values()))\n",
    "\n",
    "pca_model = PCA(n_components = 2)\n",
    "pca_model.fit(hot_vectors)\n",
    "\n",
    "hot_vectors_transformed = pca_model.transform(hot_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0021667 , -0.00597007],\n",
       "       [ 0.00317533,  0.00629301],\n",
       "       [-0.01063903,  0.00357756],\n",
       "       ...,\n",
       "       [ 0.0044176 , -0.00695686],\n",
       "       [ 0.01102729, -0.00420481],\n",
       "       [ 0.00020639,  0.00225288]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot_vectors_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11a71622e08>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12, 12))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x = hot_vectors_transformed[:500,0],\n",
    "    y = hot_vectors_transformed[:500,1],\n",
    "    s = 16,\n",
    "    color = \"red\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#... = pd.DataFrame(...[:500,], columns = ['X', 'Y']) - 0 label\n",
    "#... = pd.DataFrame(...[:500,], columns = ['X', 'Y']) - 1 label\n",
    "\n",
    "#sns.scatterplot(\n",
    "#    x = 'X',\n",
    "#    y = 'Y',\n",
    "#    data = pd.concat([(...).assign(set_type='test'), (...).assign(set_type='train')]),\n",
    "#    s = 12, hue = 'set_type'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
